[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Project-based Approach to Introductory R and Python for Data Science",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#the-data-science-workflow",
    "href": "index.html#the-data-science-workflow",
    "title": "A Project-based Approach to Introductory R and Python for Data Science",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\nRather than choosing a specific definition, you can think about data science from a process perspective. The following framework developed by Lee et al. (1), is an example of how your data science process may unfold for a given project.\n\n\n\nData Science Workflow Example\n\n\nNotice that the diagram represents the various data science processes in a non-linear way. Although the project you will pursue is structured through a sequence of milestones, it is common and often necessary to revisit prior stages of a data science workflow. For example, it may not be unusual to begin with data collection before framing a problem, and this may be driven by access to a particular data source. As explorations unfold, you might find the need to consider other angles of inquiry. On the other hand, questions of interest may drive the data collection process.\nAs we explain later, your course project will begin with a pursuit based on your interest. You will find a dataset that is of interest to you and this will be the starting point for subsequently framing a data investigation (i.e., you will consider and gather data as the initial workflow step).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-and-python",
    "href": "index.html#why-r-and-python",
    "title": "A Project-based Approach to Introductory R and Python for Data Science",
    "section": "Why R and Python?",
    "text": "Why R and Python?\nSo, why is this course focused on data science programming using R and Python?\nFor one reason why R and Python are important for data science, let’s consider the latest Kaggle report State of Data Science and Machine Learning - Kaggle 2022 that presents results from a survey of data science industry professionals from around the world. In this presentation, both Python and R are in the top three programming languages for data scientists. Furthermore, RStudio was used approximately by one in four data scientists and Jupyter Notebooks were used by over 9 out of 10 data scientists who responded to the survey.\nSince the time of this study, RStudio has integrated Python into its platform. In particular quarto files can contain both R and Python code, as well as other languages, and can render reports and presentations in many different common files types, such as pdfs, word documents, powerpoint, and more.\nSo, R and Python are not only powerful and useful languages to accomplish a given data science tasks, but they are also widely used by professional practitioners. This is for good reason - both R and Python are often found to be the intuitive tool of choice. Furthermore, individuals from all fields of study and professions desiring to derive reproducible insights from data should feel confident that the tools that are within their grasp, R and Python, are industry standards.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-outcomes",
    "href": "index.html#course-outcomes",
    "title": "A Project-based Approach to Introductory R and Python for Data Science",
    "section": "Course Outcomes",
    "text": "Course Outcomes\nOur goal in creating this book is to provide you with a resource to guide your R and Python programming skill acquisition through a data-centric project-based approach. A fundamental idea behind this approach involves facilitating your agency in continued data science learning and up-skilling. So, you will not only learn to program in R and Python, but you will also be able to extend your learning in a self-directed capacity with a broader data science workflow in mind.\nWe will provide material for you to increase your knowledge and comfort with programming in R and Python through relevant and applied data explorations and a real world data investigation experience. The R and Python programming skills that you will acquire will be utilized to create a data science project.\n\nHow can this book be used?\nThis book can be used as a course preview, review, and supplement to the material that will be covered in class. In our R and Python for Data Science course, and in other similar project-based courses, suggested readings can (and will) be provided from the contents of the following chapters in corresponding sequence with the course material. This book is also a guide to the course project, the overall course objectives, and the big picture data science considerations that we will emphasize.\nApplied data science may involve goal-oriented steps that help to move projects towards a certain or desired outcome. In the next chapter we present project development steps from this applied focus. We also include big picture considerations that can transcend a given data science project. Of course, since we are learning programming for data science through an application, we need data! But before that, we need to setup our environment in such a way that is helpful in organizing our processes and files in service of our investigation of interest.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#setting-up-your-environment",
    "href": "index.html#setting-up-your-environment",
    "title": "A Project-based Approach to Introductory R and Python for Data Science",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstalling RStudio\nIf you’re interested in using R on your local device(s), you can download the latest version of the language and the RStudio platform through the Comprehensive R Archive Network (CRAN) at The R Project for Statistical Computing Website.\n\n\nThe RStudio IDE\nRStudio is an integrated development environment (IDE). As an IDE, RStudio provides a way to create & edit code, render presentations, connect with version control platforms and more.\nLet’s look at the default layout and the RStudio desktop features that will help us build our data science workflows as we learn the R programming language.\n\n\n\nRStudio desktop\n\n\nIn the image above, there are four main windows. The upper left is the Source window where you can open various files to write, manage, and save your code. Notice that there is an “Untitled1” tab in this window, which is an unnamed R Script - a basic file type that allows you to write and manage your code. In an R script we might read in data and write code to gather information about the data, such as the size of the dataset and the column names. RStudio uses certain colors to delineate different aspects of your code (e.g., to distinguish “comments” from executable code).\nBeneath the Source window is the Console. The console displays the code that is run and the results of the code that is run. You can also type code directly into the console and even save different objects to memory from this window. However, the console does not save or retain your code once you have ended your R session, as where R Scripts and other file types managed through the Source window are designed for this purpose.\nThe window on the top right contains many tabs, including the Environment tab. The Environment tab provides information on the objects, or accessible elements, in your session’s memory. Other tabs in the window may include the History tab, and perhaps a tab that interfaces with version control platforms, like GitHub.\nThe final window, in the bottom right, also contains many tabs. The Files tab provides information about your current working directory and the files that exist within that working directory. The Plots tab can display features such as graphs generated from code, like a scatterplot or boxplot. In the image above, the information displayed in the bottom right window is from a call to the Help feature which has been done through the line of code that reads “?rbinom”. The contents of the help search will appear under the Help tab when selected.\nYour RStudio view is customizable through the Pane Layout options. For the purposes of this course, the default display will work well. Our focus will mainly be on the Source/Editor and the Console. We may reference the Files tab, the Help feature, and display plots within the Plot window; and we may investigate information within the Environment. However, these useful features supporting roles in our data science workflow which will mainly be captured by the main character - our editor.\nIn this section, we introduced a lot of vocabulary without detailed explanation. As you move through the course we expect that all of the information referenced here will be clear to you and encourage you to look back at this section, in particular, and the accompanying image as a reflection of your progress with the RStudio platform. You will be able to describe all that appears in the visual!\n\n\nSetting up your course workspace (R projects)\nOrganization is a fundamental part of a project workflow. For our project, we can leverage the computer’s folder system to efficiently and effectively store and access our course materials. Given that we are working in RStudio, we can do this by setting up an R project.\nAn R project is a directory that can contain all of your files related to an analysis or some grouping of materials that you would want to keep together. For example, your R project might contain datasets, images, scripts, output, and other information that you access in creating a presentation. For our course, the R project we create will be used to keep track of your course materials and assignments. The R project space we will create will be your organizational space that points to what you are doing in this course.\nWhen you set up your R project you will create a folder on your computer and new documents that you add to that folder will be a part of the files in your R project environment. Conversely, files that you create in your R project session will be added to your R project computer folder by default.\n\n\n\nFiles Window\n\n\n\n\n\n\n\n\nThe Files Window\n\n\n\nNote that the Files tab/window is an interface to your computer files, so modifying files through this window correspondingly modifies the files on your computer, outside of R.\n\n\n\n\n\n\n1. Lee H, Mojica G, Thrasher E, Baumgartner P. Investigating data like a data scientist: Key practices and processes. Statistics Education Research Journal (2022) 21:3–3.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "proj1R.html",
    "href": "proj1R.html",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "",
    "text": "1.1 Dataset Requirements\nWhen choosing your dataset of interest you should think about dataset characteristics that may allow you to pose dynamic and insightful questions, and will allow you to explore potential trends, patterns, and relationships. These considerations include the number of observations, the number of variables, and the types of the variables within the data.\nPlease note the following guidelines in choosing a dataset for your project.\nData accesibility is a nice feature. If everyone is able to access the data you choose they will also be able to explore and investigate the data in a potentially similar or different way.\nYour data should contain a sufficient amount of information, but also be manageable within the context of the project and course goals.\nIn order to consider various and common data types and to be able to compare, group, visualize, and perform other data moves and transformations:\nAlso consider datasets with a time component, which may give you the option to investigate or observe temporal change.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#dataset-requirements",
    "href": "proj1R.html#dataset-requirements",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "",
    "text": "Choose a publicly available dataset without restrictions or special permissions.\n\n\n\nChoose a dataset between 300 and 10,000 observations (rows).\n\n\n\nChoose a dataset with 10 to 30 variables (columns), that includes both quantitative and categorical data types. Your chosen data may also include character or text-based data types.\n\n\n\n1.1.1 Other dataset considerations\nOther considerations for choosing your dataset may include the format of the available data. For example, if you have a time feature in your data, this can be formatted as a column for each different measure across time (wide format), or as multiple rows per unit, representing different times (long format). Both formats can be useful depending on your project step and the corresponding goal, but keep in mind that some datasets may require more procedural steps than others (e.g., data wrangling).\nYou should also attempt to identify potential hurdles such as missing data. In this case, you will want to understand why some data is missing and be able to identify references that explain this occurrence. Furthermore, you should ensure that your intended data use complies with ethical guidelines. As a guide on broad considerations, designed for agencies but also applicable to individuals, (e.g., transparency, purpose specification, and others), you can reference the Fair Information Practice Principles (1).\nIf you would like your class project to involve data from your research, make sure to adhere to the guidelines above and consult your instructor for additional requirements. This may include obtaining permission from your advisor and compliance with any related research guidelines.\nOnce you find a dataset, you will need to be able to work with it! So, let’s explore some initial steps to get started using the R programming language through the RStudio platform.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#data-types-structures",
    "href": "proj1R.html#data-types-structures",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "1.2 Data Types & Structures",
    "text": "1.2 Data Types & Structures\nVoltron is a classic cartoon from the 1980s, with later versions made in the 2000s. In this cartoon various independent lions, each with unique attributes, merge together to form the Voltron Robot.\nData types and data structures are fundamental building blocks in any programming language. Data types are like categories of information that come with certain properties, such as the type of operations that can be performed on them. Data structures often involve collections of data types and allow for efficient organization, manipulation, and analysis of the various datatypes, or information contained in a given dataset.\nSo, data types are like the Voltron lions and can be combined into a data structure to create something new and functional, like the Voltron robot. As you learn more about data types and structures, consider an analogy that you can make to paint a picture of the relationship between these programming building blocks.\nR provides a range of data types, including numerical, categorical, logical and others. The main data structures in R are vectors, matrices, lists, and dataframes (or tibbles).\n\n1.2.1 Data types\nNumeric is the default quantitative data type in R. Let’s look at some examples in the code below.\n\n# (in R, variable names are flexible but are case sensitive)\n\nnum1 &lt;- 5 # Here I'm storing the value 5 in an object called num1\n\nnum.2 &lt;- pi\n\nnum_3 &lt;- .07\n\nNum4 &lt;- 10e-3\n\nEach of the numbers above (5, pi, .07, and 10e-3) are recognized as numeric data types. The integer, 5, and decimal value, .07, may be obvious candidates for a default numeric type, but what about pi and 10e-3?\nR has certain “reserved” objects that are built into the language. pi is an example and is a reserved name for value 3.141593… Likewise, 10e-3 is how R represents scientific notation. This expression is equivalent to \\(10*10^{-3}\\). In your future R use, you might encounter certain model output that reports large or small values in this scientific notation format.\nThere are a few additional very important components of the code above. Notice that each of the numeric data types have been associated with various names through the \\(\\leftarrow\\) symbol. The arrow pointing to the left is an assignment statement, which serves to store each of the numeric data types into the corresponding name. Our numbers are now objects that we can reference by name!\n\nnum1 # this is the name I gave to the value 5\n\n[1] 5\n\nNum4 # this should be 10e-3 or 0.01\n\n[1] 0.01\n\n\nFrom the executed code, you can see that calling the variables by name (or referencing them) results in their display. In general, calling an object by its name (running code that references stored information) displays its contents. Now that we know how to store and retrieve information (such as numeric data types), let’s move on to the final component of the code we’ve observed so far.\nNotice in both code snippets there are lines with the # symbol followed by descriptions. These are called code comments and are an essential part of the reproducible coding process. Within a code snippet (or code chunk/block) anything following the # symbol on a particular line does not execute. R treats all things that follow the # as comments, or notes, that aren’t part of the code that should be run.\nIn the first code chunk we made an explanatory comment to emphasize that R object names can be specified in different ways, but are case sensitive. Other restrictions on R object names can be found here: R’s Naming Rules. In the second code chunk, we made comments following the executable code to specify what we should observe in the code output.\n\n\n\n\n\n\nLooking Forward\n\n\n\nWe will emphasize the importance of code comments in the data science programming workflow throughout this book.\n\n\nNumeric data types are not just for storing numeric data. We can also perform operations on numeric data, such as addition, subtraction, exponentiation, and others.\n\n1 + 3\n\n[1] 4\n\nnum1*Num4 # 5*0.01\n\n[1] 0.05\n\nnum1^2\n\n[1] 25\n\n\nCharacter data includes non numeric information, such as strings or text data, and is specified in quotes.\nLet’s look at some examples.\n\ncharA &lt;- \"Whaz up,\"\ncharB &lt;- \"whazz upp,\"\ncharC &lt;- \"whazzz uppp!\"\n\n\n# In R, you can pass objects to a function. \n# Here is an example using the paste() function, \n# with charA, charB, and charC\n\npaste(charA, charB, charC)\n\n[1] \"Whaz up, whazz upp, whazzz uppp!\"\n\n\nNote that numbers, that would be numeric by default, can be specified as character by using quotes.\n\nA &lt;- \"7\"\nB &lt;- \"200\"\n\nA\n\n[1] \"7\"\n\n\nLogical data types in R represent Boolean values that can be either true or false. Logicals are very useful for checking conditions and are used to construct if-then statements and subsetting data, as a few examples.\n\n1 + 1 == 2\n\n[1] TRUE\n\n8 &gt; 9\n\n[1] FALSE\n\n\nNow that we’ve learned about R data types, and before we learn about R data structures, let’s pivot to a different topic that will allow us to bring tools into our R environment to facilitate both or data science learning and programming applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#packages-libraries",
    "href": "proj1R.html#packages-libraries",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "1.3 Packages & Libraries",
    "text": "1.3 Packages & Libraries\nR packages contain supplements to all of the wonderful and useful default tools that exist in base R. An R package is typically built for a particular purpose and might contain customized functions, data sources, & related documentation. An example is the ggplot2 R package that is built for data visualization. This package includes data visualization functions built on “geometries” and useful documentation for each function is included in the package for easy reference.\nR packages can be installed right through the console by using the install.packages() function, or more conveniently through the Install feature found within the Packages pane.\nOnce a package is installed it is stored in a location that we call a library. The package contents can be loaded into your current R session from this storage location, sort of like checking out a book from, well, a library. You can “check out” a package and use its contents in your current R session by referencing the package name inside the library() function.\n\n#  Example syntax of calling the library function\n#  to load the (already installed) ggplot2 package\n#  into our current R session\n\nlibrary(ggplot2)\n\n\n\n\n\n\n\nLooking Ahead\n\n\n\nIn chapter 7 will see how to use the ggplot2 package to visualize our data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#importing-data",
    "href": "proj1R.html#importing-data",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "1.4 Importing data",
    "text": "1.4 Importing data\nRecall, the file structure you set up for your R project. The folder associated with your project holds your course files. When your R project is open in your current RStudio session the folder contents can be seen and accessed through the Files pane. This means that your current “working directory” is your R project folder.\nYou can also import other files into your R session that are not necessarily in your R project by referencing them through their file path - their location on your computer. So, a dataset of a particular file type can be imported using this method whether or not it’s in your current working directory.\nFor a .csv file the base R import function is read.csv(). You would add the dataset filepath of interest, in quotes, inside of this function to import the data.\nLet’s take a look at an example using the college dataset from the ISL with R, 2nd Edition Resources. Since we learned about packages and libraries, for this example we can load the readr package from our library and use a function from this resource.\n\n# Since the dataset is in my current working\n# directory, I only need to specify the name\n# as the file path\nlibrary(readr)\ncollege &lt;- read_csv(\"College.csv\")\n\nIf you’re thinking, “but wait! We’re using RStudio. There must be another way,” then you’d be right.\nAs we mentioned before, within the Environment tab there is an Import feature that will allow you to search our computers file system for the dataset we wish to import, and this will generate the syntax for you with the function and file path built in.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#data-structures",
    "href": "proj1R.html#data-structures",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "1.5 Data Structures",
    "text": "1.5 Data Structures\n\n1.5.1 Vectors\nA vector in R is a fundamental data structure that represents a sequence of elements, all of which must be of the same type. For example, every element in a character vector is a character data type, every element in a numeric vector is a numeric data type, and so on. Vectors form the foundation for more complex data structures like matrices and dataframes (or tibbles).\nVectors are one-dimensional data structures where each element corresponds to an index value according to its position. Note that in R, the index values start at 1. This means that the element in the first position of a vector corresponds to index 1 and the element in the second position corresponds to index 2, and so on. This indexing allows for referencing vector elements or groups of elements by their location, which can be leveraged to facilitate data processing and manipulation.\nLet’s explore a few vectors by first creating one ourselves.\n\n# the c() function \"combines\" elements (separated by commas) into a vector.\neven_vec &lt;- c(2,4,6,8,10)\n\nodd_vec &lt;- even_vec - 1\n\neven_vec # display the elements of even_vec\n\n[1]  2  4  6  8 10\n\nodd_vec # display the elements of odd_vec\n\n[1] 1 3 5 7 9\n\n\nNote that in the above code, we “combined” our data values into a vector by using the c() function which is designed for this purpose. Wrapping our vector values in the c() function tells R to treat the group of values as a vector data structure.\n\n\n\n\n\n\nLooking Ahead: Vectorization\n\n\n\nIn the code above we created a vector which contained the first 5 even integers. By subtracting 1 from this vector, we were able to create a new object containing the first 5 odd integers. In other words, through the code above the subtraction operation was applied to each element of even_vec to create odd_vec. This process of applying an operation to each element of a vector without having to iterate through each one in a loop is known as vectorization.\n\n\nSo, now that we’ve created some vectors out of thin air, let’s return to the dataset we loaded into our environment to find a vector existing among its peers (other vectors!). But first, let’s take a look at some of the observations and variables in the college dataset.\n\n\n\n\n\nInstitution\nPrivate\nApps\nAccept\nEnroll\n\n\n\n\nAbilene Christian University\nYes\n1660\n1232\n721\n\n\nAdelphi University\nYes\n2186\n1924\n512\n\n\nAdrian College\nYes\n1428\n1097\n336\n\n\nAgnes Scott College\nYes\n417\n349\n137\n\n\nAlaska Pacific University\nYes\n193\n146\n55\n\n\n\n\n\n\n\nThe display shows the first 5 rows and columns of the college dataset. We can see that the variable types appear to change across the rows and remain similar within each column. The columns are our vectors!\n\n\n1.5.2 Dataframes\nA dataframe in R is a two-dimensional data structure. Dataframes can store different types of data in each column, such as numeric, character, or logical data types all in one group. We can think of a dataframe as a collection of vectors. However, for typical datasets that may be represented as a dataframe structure not only are the elements of the columns related (e.g., by representing the same data type and variable) but also the rows, which typically represent an observation of many values (or measurements, or data types) on a single unit or individual. This explains what we observed in the college dataset output.\nEach element within a dataframe is accessed using a pair of indices, one for the row and one for the column. This indexing system allows for organized data storage and facilitates access and manipulation of entire rows, columns, or specific elements within them.\nLet’s print out the first variable of the first observation.\n\n# displays element (1,1) of the dataset.\ncollege[1,1]\n\n# A tibble: 1 × 1\n  Institution                 \n  &lt;chr&gt;                       \n1 Abilene Christian University\n\n\nSince we read in the college data with the read_csv() package instead of the read.csv() package in base R, the default data structure is a tibble. Tibbles are essentially the same as dataframes.\nFinally for our dataframes and tibbles, lets observe code that will demonstrate how indices can be used to display the default output from referencing the first 5 rows and columns of our imported dataset.\n\n# gets rows 1 to 5, and gets columns 1 to 5\ncollege[1:5,1:5]\n\n# A tibble: 5 × 5\n  Institution                  Private  Apps Accept Enroll\n  &lt;chr&gt;                        &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Abilene Christian University Yes      1660   1232    721\n2 Adelphi University           Yes      2186   1924    512\n3 Adrian College               Yes      1428   1097    336\n4 Agnes Scott College          Yes       417    349    137\n5 Alaska Pacific University    Yes       193    146     55\n\n\nNotice that the data structure is listed as a tibble, and along with the data we also see each column data type between the column names and the data values.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj1R.html#a-few-more-data-structures",
    "href": "proj1R.html#a-few-more-data-structures",
    "title": "1  Project Part 1: Finding a dataset",
    "section": "1.6 A Few More Data Structures",
    "text": "1.6 A Few More Data Structures\n\n1.6.1 Matrices (aka Matrixes)\nA matrix in R is also a two-dimensional data structure where elements are arranged in rows and columns. Matrices are not as flexible as dataframes. One restriction on matrices is that all elements in a matrix must be of the same type. Due to this restriction, matrices are like two dimensional vectors.\nAs with a dataframe, each element in the matrix is accessed using a pair of indices, one for the row and one for the column. This structured indexing allows for organized storage of data and supports operations that involve manipulating entire rows, columns, or specific elements within the matrix.\n\n\n1.6.2 Lists\nLists are another type of important data structure in R. Lists are vary flexible and can have as elements entire data structures of various types. Lists can be useful in building functions and other complex tasks that require a flexible data structure. As useful as lists can be, for this course the data structure of choice is the dataframe (or tibble)!\n\n\n\n\n1. Council FP. Fair information practice principles. (n.d.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Part 1: Finding a dataset</span>"
    ]
  },
  {
    "objectID": "proj2R.html",
    "href": "proj2R.html",
    "title": "2  Project Part 2: Creating a data dictionary",
    "section": "",
    "text": "2.1 Basics in R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Part 2: Creating a data dictionary</span>"
    ]
  },
  {
    "objectID": "proj2R.html#basics-in-r",
    "href": "proj2R.html#basics-in-r",
    "title": "2  Project Part 2: Creating a data dictionary",
    "section": "",
    "text": "2.1.1 Comments\nSuppose you need to work on code for a project as a member of a team. Or, perhaps you are working on code for multiple projects and need to switch between these projects at different points of the process.\n\nWhat do you imagine are some of the challenges you might you face when working over an extended period of time?\n\nOr, what if some time has passed since you last worked on a data science project involving coding and you need to get back to it. Upon resuming your work, you might ask: “Where did I leave off with my code, and what was the purpose of this line at this point?”. In sharing your code, maybe one of your collaborators is wondering why you implemented a particular data step, function, or method at a given line.\n\nWould you want to start at the beginning of a file and retrace the logic up to the stopping point in order to understand what should be next?\n\nThis is possible and can be a beneficial review of your process, but as your project and code develop over time, this approach would become more and more time consuming. Even with a line by line review it’s possible that you wouldn’t recall all of the reasons for your coding choices and the corresponding purposes.\nA better method to facilitate effective work on coding projects over time or with collaborators, is to use clear and informative code comments. Comments are an essential part of the coding process, especially for reproducibility. In addition to tracking clear and informative information about your coding process, comments can be incorporated directly into the lines of code to benefit your understanding of your data science coding process, as well as that of others who will see or work with your code.\nAs you saw in the previous chapter, in R, code comments begin with the # character.\n\n# The text following the hashtag on this line is part of a comment\n# Here is another comment\n\nIn the midst of code, comments can be added on a different line from the code you’d like to run, or on the same line of the code you’d like to run as long as the hashtag follows the executable code. Recall, everything following the hashtag is treated like a comment.\n\n# I'm creating a vector of important numbers called Important_Numbers.\n\nImportant_Numbers &lt;- c(\"177\",\"243\",\"388\") # characters\n# These are product identifiers \n\nA scenario (comments)\nImagine your collaborator gave you code to modify in order to run some basic calculations and you were tasked with summarizing the characteristics of the quantitative values - including statistical measures such as means. Since you only read this book up to chapter 1, you decided to just input the variables into the mean() function without checking any metadata, such as variable types. You proceed to run the code shown below on the Important_Numbers variable (that is part of a larger dataset).\n\nmean(Important_Numbers)\n\nWarning in mean.default(Important_Numbers): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nTo your surprise, the code throws an error!\nLuckily, your collaborator made a note about this variable through a comment. Upon looking through the code, and without having to recognize the quoted numbers (that were embedded in a much larger code sequence), you are able to easily see that the variable Important_Numbers was composed of character values, intentionally, to reflect their designation as product identifiers. Since you know you (and R) cannot compute the mean of a character vector you were able to identify this as the source of the error! Furthermore, from the helpful comment, you understand the reason why these numbers are being treated as character values. Upon reflection, you realize that your thoughtful collaborator helped you facilitate the debugging process and also provided you with additional information about the variable through their informative code commenting practices. You remove Important_Numbers from the calculations, check the metadata of the other variables, and proceed with the calculations on the appropriate information, error free.\nWe can also imagine a scenario where some lines of code may be in an unfamiliar syntax (R has so many ways to perform the same task), and your best way of gaining insight into the purpose of the code may likely be through having access to informative code comments. Also, keep in mind that R errors are not always as specific as the output we see above and may appear apart from a particular line of code.\nNow that we know how to make comments within code and can see how they can be useful, we will continue to use them as an important and essential part of our data science coding, collaboration, and reproducibility workflow.\n\n\n2.1.2 Variables\nIn a typical dataset, variables are often measurements or characteristics of observations. Observations are some unit on which measurements are taken. For example, “height” (measured in feet), and “primary language” might be recorded for people who live in Wake County, North Carolina. In this case, we would call “height” and “primary language” variables, and the different people who live in Wake County whose heights and primary languages were recorded would be the observational units that make up the observations in the dataset. Variables are usually represented by columns within a dataset, and each observation (represented by the rows) has an associated set of variable measurements, or information.\nBy contrast, in R a variable is an object that has some value or values associated with it. In the R programming context, the use of the term variable is more general. In the programming context case, for example, a variable may contain an entire dataset (and the associated rows and columns). The different uses of this term are important to note since what is meant by the term “variable” may depend on the context.\n\n\n\n\n\n\nVariables (and terminology) vary!\n\n\n\nWe may use the term variable to refer to an object that can be referenced and has general information associated with it (the programming context), or as a column in a dataset containing information about observations (the data science context).\n\n\n\n\n2.1.3 Assignment statements\nConsider the general programming use of the term variable: How can we create a variable and how can we reference it? As you’ve already seen, the answer to this question is to store your created object into a reference via an assignment statement.\nIn R, you can use two different symbols to assign information to a variable. These are &lt;- and =. It is also possible to make assignment statements from left to right using -&gt;, but R programmers tend to use &lt;-, specifically. This standard left pointing arrow is what we will use as an assignment statement throughout this book.\nFor a glimpse into the history of the R assignment statement see the resource here: Why do we use arrow as an assignment operator?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Part 2: Creating a data dictionary</span>"
    ]
  },
  {
    "objectID": "proj2R.html#data-dictionaries",
    "href": "proj2R.html#data-dictionaries",
    "title": "2  Project Part 2: Creating a data dictionary",
    "section": "2.2 Data Dictionaries",
    "text": "2.2 Data Dictionaries\nWe mentioned the benefit of code comments in collaborative processes. Even before the data-focused coding process begins, a more important workflow step involves creating a data dictionary. Data dictionaries are to datasets as code comments are to code.\nData sources may come with varying levels of background information. We can refer to this information about our data as metadata. This background information on a dataset is essential to the steps of an associated investigation of the data. For example, as you progress towards later stages of the course project where you will communicate insights from data, consider how you might go about this communication without knowing the data source (or generating mechanism), or the information on variable ranges, categories, or other useful descriptors. If you didn’t know this information, how could you hope to communicate it to others?\nFor the course project and the dataset you choose, think of yourself as the data curator. Part of your responsibility in assuming this role is to create a data dictionary. The objective of creating the data dictionary is for you to be able to hand it and the dataset it describes to someone else and that person would have all the information they need to understand where the data is from, what the data contains, and what the data is about, including context, possible uses, limitations and more.\n\n\n\n\n\n\nConnection to the Data Science Workflow\n\n\n\nRecall the reference to the data science workflow in the introduction. Considering the categories of this particular framework, the development of the data dictionary could be a part of many of the stages. However, more importantly than situating the data dictionary in a particular workflow stage is recognizing how important the information within the data dictionary is to all stages. Although not all data dictionaries are created equal, the ones that we will create are designed to inform and impact the entire data science workflow!\n\n\n\n2.2.1 Data dictionary requirements\nLet’s consider some essential components of the data dictionary that you will create for your course project.\n\nBackground Information\n\nDescribe what your dataset is about (e.g., what are the observations and what’s being measured and/or characterized)\nIdentify the source of your data.\n\nGeneral Metadata\n\nList the dimensions of the dataset.\nAre there missing values? If so, explain why there are missing values and how the missing values are coded in the dataset.\n\nVariable Characteristics\n\nList and describe the variables in your dataset.\nFor categorical variables, list the levels and for quantitative variables give the units of measure and the data ranges.\nDescribe whether your data contains unique identifiers and other variable types such as character or text data and what these variables represent.\n\nOther considerations:\n\nDescribe how the data was collected or generated.\nDescribe by whom the data was collected and the purpose and intended use case (e.g., what questions were asked and what answers were sought) for the data that was collected.\nWhat are the limitations of the data, including information that may be missing from the data dictionary.\nConsider what additional information may be useful for others to know about the data and include this when appropriate.\n\n\nGiven that you are not designing the data collection process, and the data you access may have prior documentation shortcomings, all of the criteria above may not be knowable or accessible. This would be an inherent limitation but still can be described in your data dictionary. In such cases you should recognize the incomplete information and think about how you could avoid such omissions in your future data curation roles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Part 2: Creating a data dictionary</span>"
    ]
  },
  {
    "objectID": "proj2R.html#describing-your-data",
    "href": "proj2R.html#describing-your-data",
    "title": "2  Project Part 2: Creating a data dictionary",
    "section": "2.3 Describing Your Data",
    "text": "2.3 Describing Your Data\nR has many functions built in to the base version of the language. You have already seen examples, such as read.csv() and paste(), and likely have a good idea about what R functions do. Still, it can be useful to recognize that R functions tend to work and look like those you may have seen in something like a math class. Like a mathematical function, an R function takes in parameters inside parentheses (e.g., a file path) and returns specific results (e.g., a dataframe). In regard to describing our data, we can leverage R functions to make the process both more efficient and insightful.\n\n\n\n\n\n\nLooking Ahead - R Functions\n\n\n\nBeyond the base R functions, supplemental functions often exist in R packages that can be installed. However, there is sometimes a need for user-defined functions which we can customize and build, ourselves, if and when needed. We will see some examples of user-defined functions in subsequent chapters.\n\n\nNext, let’s consider an example of how we can describe our data and create & verify our data dictionary.\n\n2.3.1 Example: Data steps for data dictionaries\nThe Marine6 dataset contains 815 observations of six marine species across 10 North American Regions from the years 1970 to 2020. Measures include location information (longitude, latitude, and depth) for certain species observed within a particular region, within a particular year. This dataset represents a subset of the data seen at the EPA - Climate Change Indicators website.\nThe paragraph above is a minimal example of the background information that should be a part of your data dictionary. To expand upon the background description and address other components of the data dictionary criteria, we will read in the Marine6 dataset and use some functions to access general metadata and information on the variables within.\n\n# Loading in the data, using the readr package\nlibrary(readr)\nMarine6 &lt;- read_csv(\"Marine6.csv\")\n\nAs noted above, we want to include the dimensions of the data in our data dictionary. Even if the information is given with the data, or is perhaps known from a process like viewing the data in a spreadsheet, we can use the dim() function to quickly figure this out and verify that the data that was read in matches the expected size. We noted above in the description that there should be 815 observations (rows) in the data. Let’s confirm.\n\n# check the dimensions of the dataset\ndim(Marine6)\n\n[1] 815  10\n\n\nConfirmed! The dim() function allows us to quickly and easily see that we have 815 rows and 10 columns in our data. The 815 rows were expected, but only three measures (latitude, longitude, and depth) were mentioned in the description. So, what’s up with 10 columns, then?\nSince we’re using R - that’s right you guessed it - there are many ways to inquire about this. The code below demonstrates one way that is particularly helpful.\n\n# What are the variables in the data named?\ncolnames(Marine6)\n\n [1] \"Year\"                     \"Region\"                  \n [3] \"Species\"                  \"Latitude\"                \n [5] \"Latitude Standard Error\"  \"Longitude\"               \n [7] \"Longitude Standard Error\" \"Depth\"                   \n [9] \"Depth Standard Error\"     \"Common Name\"             \n\n\nUsing the colnames() function, from the output it’s now clear that six of the 10 measures were described in the background information (the three location measures, Year, Region, and Species). We can also see that along with the location measures there are associated standard errors for each, and one additional variable called “Common Name”. But let’s dive deeper to find out even more about the variables in the Marine6 dataset.\n\n# Let's get default summaries for the variable\nsummary(Marine6)\n\n      Year         Region            Species             Latitude    \n Min.   :1970   Length:815         Length:815         Min.   :26.51  \n 1st Qu.:1991   Class :character   Class :character   1st Qu.:32.40  \n Median :2000   Mode  :character   Mode  :character   Median :37.92  \n Mean   :1999                                         Mean   :37.54  \n 3rd Qu.:2010                                         3rd Qu.:42.23  \n Max.   :2020                                         Max.   :57.50  \n                                                      NA's   :5      \n Latitude Standard Error   Longitude       Longitude Standard Error\n Min.   :0.0000          Min.   :-159.98   Min.   :0.0000          \n 1st Qu.:0.1770          1st Qu.: -79.90   1st Qu.:0.2362          \n Median :0.2267          Median : -75.05   Median :0.3298          \n Mean   :0.2942          Mean   : -76.79   Mean   :0.3859          \n 3rd Qu.:0.3342          3rd Qu.: -68.63   3rd Qu.:0.4475          \n Max.   :4.4575          Max.   : -60.24   Max.   :1.7813          \n NA's   :34              NA's   :5         NA's   :34              \n     Depth        Depth Standard Error Common Name       \n Min.   :  7.48   Min.   : 0.0000      Length:815        \n 1st Qu.:  8.43   1st Qu.: 0.1287      Class :character  \n Median : 52.98   Median : 4.4436      Mode  :character  \n Mean   : 67.22   Mean   : 5.7448                        \n 3rd Qu.:119.63   3rd Qu.: 9.2985                        \n Max.   :255.06   Max.   :63.2701                        \n NA's   :53       NA's   :73                             \n\n\nThere is a lot to take in about the output here, so let’s examine what information the summary() function is giving us. It appears that for the variables that we might assume to be numeric, the summary function returns a “5 number summary” which includes the quartiles, a mean value and something else that says “NA’s”. This is the standard output of the summary function for variables that are of type numeric. The “NA’s” in the output represents a count of missing values for the respective variable.\nNotice that for the other non-numeric variables we just get a length, and then the output “character” for the class and mode. This is the default summary function output for variables of type character, and it’s not very useful in learning about the contents of the variable. However, we can fix that!\nThe variables “Region”, “Species”, and “Common Name” contain classifications or categories of information. We can treat these types of varibles as factors. As an example of a factor variable, let’s consider a variable called “Color”. Let’s say that in the dataset that included this color variable, each observation could be classified as “red”, “blue”, or “green”, meaning that each observation that has a color value recorded would have a corresponding label within the [“red”,“blue”,“green”] set of colors. If we designated “Color” as a factor, then the [“red”,“blue”,“green”] set would be what we call the levels of this factor variable.\nLet’s see what happens in the summary if we consider our character values as factors. In the code below, we will use the $ operator to extract the “Region” variable from the Marine6 data and use a nested function statement to input this variable into summary() as a factor.\n\n# getting the summary of the variable\n# \"Region\" in the Marine6 dataset\n# treated as a factor\n\nsummary(as.factor(Marine6$Region))\n\n            Gulf of Alaska             Gulf of Mexico \n                        12                         74 \nGulf of St. Lawrence South           Maritimes Summer \n                        48                         99 \n         Northeast US Fall        Northeast US Spring \n                       164                        132 \n         Southeast US Fall        Southeast US Spring \n                        87                         92 \n       Southeast US Summer          West Coast Annual \n                        93                         14 \n\n\nAha! This output is much more useful. In particular, we can now see the levels of the “Region” variable and see that there are 10 different regions represented in the data, as described in the background information. The numbers below the region levels are the counts. We can see that there are 12 measurements for the “Gulf of Alaska” region among the other regions in the data.\nBefore we move on, let’s revisit the line of code that led to this output. We used two functions and one operator simultaneously. Recall, our mathematical functions analogy. Just as with math functions, you can use function nesting or composition with R functions. The $ operator is the extract operator. So, in this code we extracted the Region variable from the data and nested the factor conversion function, as.factor(), within the summary() function to create the output of interest. What we observed in the output is what the summary function returns by default when the input is of type factor.\nSince the factor version of our character variables returns useful summary information, let’s go ahead and update the data so that this is displayed for each of the variables that we would like to consider this way.\n\n# convert characters to factors (one.. at.. a.. time)\nMarine6$Region &lt;- as.factor(Marine6$Region)\nMarine6$Species &lt;- as.factor(Marine6$Species)\nMarine6$`Common Name` &lt;- as.factor(Marine6$`Common Name`)\n\nLet’s see what the new summary gives us.\n\n# summaries of our quantitative and categorical variables\nsummary(Marine6)\n\n      Year                      Region                     Species   \n Min.   :1970   Northeast US Fall  :164   Centropristis striata:180  \n 1st Qu.:1991   Northeast US Spring:132   Homarus americanus   :187  \n Median :2000   Maritimes Summer   : 99   Larimus fasciatus    :158  \n Mean   :1999   Southeast US Summer: 93   Orthasterias koehleri: 26  \n 3rd Qu.:2010   Southeast US Spring: 92   Sphyraena guachancho :126  \n Max.   :2020   Southeast US Fall  : 87   Urophycis chuss      :138  \n                (Other)            :148                              \n    Latitude     Latitude Standard Error   Longitude      \n Min.   :26.51   Min.   :0.0000          Min.   :-159.98  \n 1st Qu.:32.40   1st Qu.:0.1770          1st Qu.: -79.90  \n Median :37.92   Median :0.2267          Median : -75.05  \n Mean   :37.54   Mean   :0.2942          Mean   : -76.79  \n 3rd Qu.:42.23   3rd Qu.:0.3342          3rd Qu.: -68.63  \n Max.   :57.50   Max.   :4.4575          Max.   : -60.24  \n NA's   :5       NA's   :34              NA's   :5        \n Longitude Standard Error     Depth        Depth Standard Error\n Min.   :0.0000           Min.   :  7.48   Min.   : 0.0000     \n 1st Qu.:0.2362           1st Qu.:  8.43   1st Qu.: 0.1287     \n Median :0.3298           Median : 52.98   Median : 4.4436     \n Mean   :0.3859           Mean   : 67.22   Mean   : 5.7448     \n 3rd Qu.:0.4475           3rd Qu.:119.63   3rd Qu.: 9.2985     \n Max.   :1.7813           Max.   :255.06   Max.   :63.2701     \n NA's   :34               NA's   :53       NA's   :73          \n               Common Name \n American lobster    :187  \n Banded drum         :158  \n Black sea bass      :180  \n Guachanche barracuda:126  \n Rainbow star        : 26  \n Red hake            :138  \n                           \n\n\nNow we can successfully talk about the information that is within the categorical variables in the data. You already knew “there are so many ways to do a thing in R,” so this is only one (perhaps inefficient) way to change the Marine6 character variables to type factor. As another method, we could have initially modified the way the data was read in to automatically convert character variables to factors, or we could have even considered a fancier way (e.g., those user-defined functions we’ll talk about soon). However, when there is a manageable amount of things to modify, sometimes the quickest way is a line by line method as shown.\nJust to be clear, in the code above, prior to the latest summary, we overwrote the character variables with their factor versions and re-saved them into themselves. Here is the translation for the line Marine6$Region &lt;- as.factor(Marine6$Region): Convert the variable “Region” in the Marine6 dataset into a factor and store it into the variable Region in the Marine6 dataset. There are many reasons why certain ways of doing may be better than others, but one takeaway for the time being is that R code runs in sequence and this has implications for code that overwrites certain objects.\n\n\n\n\n\n\nLooking Ahead\n\n\n\nThe tidyverse contains the dplyr package with data processing functions that can be leveraged to write efficient code and minimize overwriting, storage, and other coding inefficiencies that may result from solely using base R.\n\n\nAnother useful base R data descriptive-focused function is the str() function which includes the data dimensions, the variable types, example values and more. Also, recall the dataset output referenced in project step one. In the same way as viewing the first few rows of data through index references, the head() function, with a dataset as input, can be used to help you “see what the data looks like”. Adding a few example rows of data to your data dictionary can also be helpful and the head function can provide this information.\nYou can now imagine ways to go about extracting and verifying the information for your data dictionary, which will often be a combination of researching and data descriptive-focused programming. As in the case where some information is not knowable from prior documentation, there are some components of the data dictionary criteria that may look different depending on the dataset you find. For example, it is not reasonable or necessarily helpful to list out every single level of a factor if there are too many (tens of levels or more). For instance, in the Marine6 dataset, instead of listing out every level of the “Region” variable, we might just describe this as “a variable with 10 levels representing North American bodies of water during different seasons.” On the other hand, we might enumerate all six levels of the “Common Name” variable.\nNote that in creating your data dictionary, the format is important. When summarizing your variables include descriptions and explanation and not just output from the summary function. Your data dictionary should tell a story about the data and is not just a replication of data summaries. As you progress in your data investigation, you can always return to your data dictionary for iterative updates, corrections, or improvements.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Part 2: Creating a data dictionary</span>"
    ]
  },
  {
    "objectID": "proj3R.html",
    "href": "proj3R.html",
    "title": "3  Project Part 3.1: Diving into Data Exploration - R",
    "section": "",
    "text": "3.1 Data Moves\nTo begin exploring your data in more detail, you will likely need to modify your dataset in some way. This may involve removing variables or observations, creating groupings or categories, performing operations on certain values, and other transformations of the data. For example, if you want to understand or visualize a particular trend in your data, you may need to index your observations in a particular order. This might be a necessary step if your data has a temporal component and you want to visualize a trend or phenomenon with respect to time.\nErickson et al. (1), refer to such data transformations as “data moves” and understanding what these are can facilitate your data science explorations and investigations regardless of your programming language or platform of choice. We will consider data moves as a fundamental part of our programming and as a guide to facilitating the many aspects of our data exploration and investigations process, from data cleaning to data visualization and beyond. Our version of data moves will be connected to the dataframe object structure that is part of both R and Python. These concepts are the SCUBA gear that will allow us to wade through the depths of our data with purpose and confidence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project Part 3.1: Diving into Data Exploration - R</span>"
    ]
  },
  {
    "objectID": "proj3R.html#data-moves-in-base-r",
    "href": "proj3R.html#data-moves-in-base-r",
    "title": "3  Project Part 3.1: Diving into Data Exploration - R",
    "section": "3.2 Data Moves in Base R",
    "text": "3.2 Data Moves in Base R\nTypical dataset modifications involve data moves such as filtering, subsetting, grouping, and merging. In chapter 3, we looked at the Marine6 dataset. This 815x10 dimension dataset was created through a series of data moves from a larger dataset consisting of 48,237 rows and nine columns. Creating this dataset of interest involved data exploration and data moves such as filtering, subsetting, grouping, and creating a new variables.\n\n3.2.1 Filtering\nThe filtering data move is what we will use to reduce or examine a dataset based on certain row criteria. Since we have information about the range of years in the dataset, from our data dictionary, we might wonder “How many observations were recorded from the beginning, in 1970?”.\n\n# read in the Marine6 dataset using base R\n# convert characters to factors upon reading in\nMarine6 &lt;- read.csv(\"Marine6.csv\", stringsAsFactors = TRUE )\n\n# find the minimum value for the variable \"Year\"\n# aka, the earliest recordings\n\nmin(Marine6$Year)\n\n[1] 1970\n\n\nIn the code above, we read in the Marine6 data and used the min() function on the “Year” variable to get the earliest date. The output confirms that the earliest year recoded in the dataset was 1970. We’ve identified our row criteria on which we will filter the data to investigate our question of interest.\n\n# filter the data set to observations of interest\n# get the number of observations\nnrow(Marine6[Marine6$Year == 1970,])\n\n[1] 2\n\n\nIn the code above we used the logical statement Marine6$Year == 1970 in the row index position to filter the Marine6 dataset (e.g., get all rows in the dataset where the logical statement is true). This output served as the input for the nrow() function which counts the number of rows and returned the answer to our inquiry.\nLike the briefly mentioned process of creating the Marine6 dataset, in many instances, we might filter our data to create a new object, or dataset, of interest. We might store all observations according to a specific set of years.\n\n# Store observation from 1970 to 2000 in M6_2000\nM6_2000 &lt;- Marine6[Marine6$Year &lt;= 2000,]\nsummary(M6_2000$Year) # max should be 2000\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1970    1983    1991    1989    1996    2000 \n\n\n\n\n3.2.2 Subsetting\nWe can use column criteria to reduce our data to only certain variables of interest. We can distinguish the subsetting data move from filtering based on the use of column vs. row criteria, although both operations result in what can be considered subsets of the data.\n\n# summary only of the subset of variables of interest\nsummary(Marine6[,c(1,2,8,10)])\n\n      Year                      Region        Depth       \n Min.   :1970   Northeast US Fall  :164   Min.   :  7.48  \n 1st Qu.:1991   Northeast US Spring:132   1st Qu.:  8.43  \n Median :2000   Maritimes Summer   : 99   Median : 52.98  \n Mean   :1999   Southeast US Summer: 93   Mean   : 67.22  \n 3rd Qu.:2010   Southeast US Spring: 92   3rd Qu.:119.63  \n Max.   :2020   Southeast US Fall  : 87   Max.   :255.06  \n                (Other)            :148   NA's   :53      \n               Common.Name \n American lobster    :187  \n Banded drum         :158  \n Black sea bass      :180  \n Guachanche barracuda:126  \n Rainbow star        : 26  \n Red hake            :138  \n                           \n\n\nIn the code above, we used a vector of indices to get a subset of the Marine6 variables. In larger datasets, particularly for gleaning insights from the summary() function, subsetting data in this way can render the output more informative and digestible.\n\n\n3.2.3 Grouping (Loop Example)\nThe grouping data move may serve various purposes, but is often used to create categories for the purposes of comparisons. Often, the grouping data move is a specific case of creating a new variable or attribute, particularly when the grouping information is needed for subsequent analysis. For example, a new variable containing grouping information may be a parameter that is used to create a data visualization in a certain way.\nIn the Marine6 dataset, we have species of fish and species of not-fish. We might be interested in comparing the changes in depths over time between these more general classifications. To set this comparison up, we can use grouping.\n\n# Using a for loop to create a new group\nfor(i in 1:nrow(Marine6))\n{\n  if(Marine6$Common.Name[i] %in% c(\"American lobster\",\"Rainbow star\"))\n  { Marine6$Group[i] = \"Other\" }\n  else if(is.na(Marine6$Common.Name[i])) \n  { Marine6$Group[i] = NA } \n  else \n  { Marine6$Group[i] = \"Fish\" }\n}\nMarine6$Group &lt;- as.factor(Marine6$Group)\nsummary(Marine6$Group)\n\n Fish Other \n  602   213 \n\n\nLet’s examine the code above, which contains a for loop. This programming sequence iterates through each observation in the Marine6 data and returns the values “Other”, “NA”, or “Fish” based on the specified condition-criteria. In the first condition we introduced the %in% operator which is a logical statement that returns TRUE if the value of “Common.Name” matches any of the characters in the object c(\"American lobster\",\"Rainbow star\"), or FALSE otherwise. The next condition uses is.na(), also a logical statement, which returns TRUE if the value of “Common.Name” is missing, and FALSE otherwise. Although there are no missing values in “Common.Name”, this may not always be the case for a given variable and we should account for this missing value condition in order to run our loop without error. Finally, the last remaining case includes all other common names not specified in the first condition. We do not need to explicitly write this out as it is included in the else statement. Note how our data dictionary can provide this factor level information to us. We may even revise our data dictionary to include information about the levels that correspond to fish and those that do not.\n\n\n3.2.4 Additional Data Moves\nIn some cases, there may be information spread across multiple datasets that we want to combine into a single dataframe. If each of these dataset has a common “identifier” which links the information, we can use the merging data move to accomplish this task. For example, we might want to add instructor information to student data, based on a common course ID. We can merge data with the base R merge() function where the inputs would be each dataset and the common identifier on which the merge would be based.\nThe creating hierarchy data move may not be different from a combination of grouping and creating a new variable. However, certain models are based on data hierarchy, and depending on the dataset there may be a need to create this structure for a related analysis. For example, we may have a dataset that contains information on states, counties, and schools. Across the states there may be counties (and schools) that have the same names. In order to distinguish one county from another, information about the state would be necessary. Thus, we would nest our counties within the states (and the schools within the counties), and this nesting would create a hierarchical data structure. As with comparing our fish and non-fish groups, we might be interested in visualizing the variation in county test scores across a sample of states, and the hierarchical information would be essential to such a task.\nIn the examples above, we used based R and bracket notation (i.e., referencing indices through conditions in the row aand column spaces of our dataframe) to perform our data moves. Needless to say, there are more ways in R to perform our data moves. In fact, many data moves directly correspond to functions that exist through the dplyr package in the “tidyverse”. Even better, these functions can really simplify our coding process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project Part 3.1: Diving into Data Exploration - R</span>"
    ]
  },
  {
    "objectID": "proj3R.html#tidyverse-data-moves---tidy-moves",
    "href": "proj3R.html#tidyverse-data-moves---tidy-moves",
    "title": "3  Project Part 3.1: Diving into Data Exploration - R",
    "section": "3.3 Tidyverse Data Moves - Tidy Moves",
    "text": "3.3 Tidyverse Data Moves - Tidy Moves\nIn the previous section, we demonstrated data moves one at a time. We could have easily added both row-filtering and column-subsetting information at the same time to create our new dataset of interest. These do not need to be independent steps, but combining these steps into one may lead to rather long code statements that could potentially be visually difficult to parse (and revise). Let’s see an example of this.\n\n# summary of a subset of variables \n# for the observation prior to 2001 and in one region of interest\nsummary(Marine6[Marine6$Year &lt;= 2000 & \n                  Marine6$Region == \"Northeast US Fall\", c(1,2,8,10)])\n\n      Year                             Region       Depth       \n Min.   :1974   Northeast US Fall         :93   Min.   : 21.50  \n 1st Qu.:1980   Gulf of Alaska            : 0   1st Qu.: 34.83  \n Median :1987   Gulf of Mexico            : 0   Median : 89.55  \n Mean   :1987   Gulf of St. Lawrence South: 0   Mean   : 75.41  \n 3rd Qu.:1995   Maritimes Summer          : 0   3rd Qu.:108.07  \n Max.   :2000   Northeast US Spring       : 0   Max.   :134.87  \n                (Other)                   : 0   NA's   :1       \n               Common.Name\n American lobster    :27  \n Banded drum         :12  \n Black sea bass      :27  \n Guachanche barracuda: 0  \n Rainbow star        : 0  \n Red hake            :27  \n                          \n\n\nIn the code above, we added two conditions into the row space to filter by observations before 2001 and only in the “Northeast US Fall” region. We also added the column space index subsetting criteria, all in the same statement. This accomplishes the filtering and subsetting task of interest in short order, but we can certainly improve on the readability of this code. Also, it’s not ideal to have to revisit column information to find out which column names correspond to which indices. So, is there a better - or at least different - way?\n\n3.3.1 A different way (and the pipe operator)\nThe tidyverse that we previewed in chapter 3 contains a very handy package called dplyr. One of the cool things about the dplyr package is that many of the functions that exist within it can be put into direct correspondence with our data moves! Before we jump into the these functions, we need an essential tool called the pipe operator, which we can get from the magrittr package.\n\n# I can add two seperate statements on one line using \";\"\nlibrary(magrittr); library(dplyr)\n\n#example using the pipe operator\nMarine6 %&gt;% summary()\n\n      Year                      Region                     Species   \n Min.   :1970   Northeast US Fall  :164   Centropristis striata:180  \n 1st Qu.:1991   Northeast US Spring:132   Homarus americanus   :187  \n Median :2000   Maritimes Summer   : 99   Larimus fasciatus    :158  \n Mean   :1999   Southeast US Summer: 93   Orthasterias koehleri: 26  \n 3rd Qu.:2010   Southeast US Spring: 92   Sphyraena guachancho :126  \n Max.   :2020   Southeast US Fall  : 87   Urophycis chuss      :138  \n                (Other)            :148                              \n    Latitude     Latitude.Standard.Error   Longitude      \n Min.   :26.51   Min.   :0.0000          Min.   :-159.98  \n 1st Qu.:32.40   1st Qu.:0.1770          1st Qu.: -79.90  \n Median :37.92   Median :0.2267          Median : -75.05  \n Mean   :37.54   Mean   :0.2942          Mean   : -76.79  \n 3rd Qu.:42.23   3rd Qu.:0.3342          3rd Qu.: -68.63  \n Max.   :57.50   Max.   :4.4575          Max.   : -60.24  \n NA's   :5       NA's   :34              NA's   :5        \n Longitude.Standard.Error     Depth        Depth.Standard.Error\n Min.   :0.0000           Min.   :  7.48   Min.   : 0.0000     \n 1st Qu.:0.2362           1st Qu.:  8.43   1st Qu.: 0.1287     \n Median :0.3298           Median : 52.98   Median : 4.4436     \n Mean   :0.3859           Mean   : 67.22   Mean   : 5.7448     \n 3rd Qu.:0.4475           3rd Qu.:119.63   3rd Qu.: 9.2985     \n Max.   :1.7813           Max.   :255.06   Max.   :63.2701     \n NA's   :34               NA's   :53       NA's   :73          \n               Common.Name    Group    \n American lobster    :187   Fish :602  \n Banded drum         :158   Other:213  \n Black sea bass      :180              \n Guachanche barracuda:126              \n Rainbow star        : 26              \n Red hake            :138              \n                                       \n\n\nIn the code above, the %&gt;% symbol is the operator of interest. The pipe operator applies the operations that follows (to the right) to the inputs that precede it (to the left). Just to understand a little more about this useful tool, let’s briefly step away from our dataset context to see another example of how the pipe operator works.\n\n# What's the stadard deviation of [2,6,10]?\nc(2,6,10) %&gt;% var() %&gt;% sqrt()\n\n[1] 4\n\n\nIn the example above, we applied the variance function var() to [2,6,10], and took the square root of the variance output by applying the sqrt() function. This is the same as sqrt(var(c(2,6,10))), but (not so) arguably easier to read, and definitely less prone to parentheses errors. Now that we’ve seen the pipe operator, let’s use it to smooth out those data moves!\n\n\n3.3.2 Filtering 2.0\nPreviously, using base R, we filtered the Marin6 dataset in order to count the number of observations that corresponded only to the year 1970. Let’s try this using the appropriate dplyr method, aptly known as the filter() function.\n\nMarine6 %&gt;% \n  filter(Year == 1970) %&gt;% #I can just name the variable\n  nrow() #counting the previously filtered data\n\n[1] 2\n\n\nWow! Above we were able to count the number of observations corresponding to the year 1970 using filter() together with the pipe operator. This new format flexibility also allows for in line commenting relative to each step.\n\n\n3.3.3 Subsetting 2.0\nNext, in base R, we used bracket notation and variable indices to subset our data (to produce a summary of interest). Let’s take a look at how we can go about accomplishing this subsetting data move (and summary) using the select() function.\n\n# a summary of the selected variables\nMarine6 %&gt;%\n  select(Year, Region, Depth, Common.Name) %&gt;%\n  summary()\n\n      Year                      Region        Depth       \n Min.   :1970   Northeast US Fall  :164   Min.   :  7.48  \n 1st Qu.:1991   Northeast US Spring:132   1st Qu.:  8.43  \n Median :2000   Maritimes Summer   : 99   Median : 52.98  \n Mean   :1999   Southeast US Summer: 93   Mean   : 67.22  \n 3rd Qu.:2010   Southeast US Spring: 92   3rd Qu.:119.63  \n Max.   :2020   Southeast US Fall  : 87   Max.   :255.06  \n                (Other)            :148   NA's   :53      \n               Common.Name \n American lobster    :187  \n Banded drum         :158  \n Black sea bass      :180  \n Guachanche barracuda:126  \n Rainbow star        : 26  \n Red hake            :138  \n                           \n\n\nNotice that we can specify the variable names as they are in the dataset. This is preferable to using indices which can lack sufficient information about what the subset should actually contain.\n\n\n3.3.4 Grouping 2.0\nThe grouping data move may serve various purposes and in some cases it is not necessary to create new variables to group data for comparisons. For example, in the dplyr package, we can use group_by() to create a “grouped” data frame where subsequent functions are applied to each group.\n\nMarine6 %&gt;%\n  group_by(Common.Name) %&gt;%\n  count()\n\n# A tibble: 6 × 2\n# Groups:   Common.Name [6]\n  Common.Name              n\n  &lt;fct&gt;                &lt;int&gt;\n1 American lobster       187\n2 Banded drum            158\n3 Black sea bass         180\n4 Guachanche barracuda   126\n5 Rainbow star            26\n6 Red hake               138\n\n\nIn the code above, we were able to generate a comparison of counts for the variable on which the grouping was based. We input our grouped data frame (created by group_by()) into the count() function to generate a summary similar to what we got through applying the summary() function to as.factor(Marine6$Common.Name).\n\n\n3.3.5 A few more dplyr examples\nRecall the rather involved base R loop that we created for the purpose of adding a grouping variable to our Marine6 dataset. Let’s consider how we could do this in the dplyr setting.\n\n# function to categorize the different common names\nfishCat &lt;- function(x)\n{ #x is the dataset variable of interest\n  ifelse(is.na(x), NA, \n         ifelse(x %in% c(\"American lobster\",\"Rainbow star\"),\n                \"Other\",\"Fish\"))\n}\n\n# creating a new grouping variable  \nMarine6 %&gt;%\n  mutate(Group = fishCat(Common.Name)) %&gt;%\n  group_by(Group) %&gt;%\n  count()\n\n# A tibble: 2 × 2\n# Groups:   Group [2]\n  Group     n\n  &lt;chr&gt; &lt;int&gt;\n1 Fish    602\n2 Other   213\n\n\nIn the code above we created a user defined function (specific to our Marine6 data context) that takes in a dataset variable and returns either NA, “Fish,” or “Other.” The function uses nested ifelse() statements to account for the various criteria, rather than the more general if-else statement that we used for the loop in our base R grouping example. Although this function does not require anything beyond base R, we at least are able to see, in case you hadn’t heard, that there is more than one way to do a thing in R. In addition, ifelse() is a vectorized function and applies the specified conditions to each element simultaneously for a more efficient process!\nBelow the user-defined fishCat() function we introduced the mutate() function. This function allows us to create a new variable from one that exists in the referenced dataset. Just as with our base R example, we named this variable “Group.” We used fishCat() to create the values of “Group” based on the components of “Common.Name”. Finally, we applied the group_by() function and counted the frequencies within each of our newly created “Group” categories.\nNow, lets look at one last function that will combine our filtering and grouping into one sequence of pipes and operations.\n\nMarine6 %&gt;%\n  filter(Year &lt;= 2000, Region == \"Northeast US Fall\") %&gt;% #filtering\n  select(Year, Region, Depth, Common.Name) %&gt;% #subsetting\n  summary()\n\n      Year                             Region       Depth       \n Min.   :1974   Northeast US Fall         :93   Min.   : 21.50  \n 1st Qu.:1980   Gulf of Alaska            : 0   1st Qu.: 34.83  \n Median :1987   Gulf of Mexico            : 0   Median : 89.55  \n Mean   :1987   Gulf of St. Lawrence South: 0   Mean   : 75.41  \n 3rd Qu.:1995   Maritimes Summer          : 0   3rd Qu.:108.07  \n Max.   :2000   Northeast US Spring       : 0   Max.   :134.87  \n                (Other)                   : 0   NA's   :1       \n               Common.Name\n American lobster    :27  \n Banded drum         :12  \n Black sea bass      :27  \n Guachanche barracuda: 0  \n Rainbow star        : 0  \n Red hake            :27  \n                          \n\n\n\n\n\n\n\n\nConsider\n\n\n\nIn our 2.0 tidyverse comparisons vs. the base R versions, the readability feature of applying data moves with dplyr may have come into focus. As you continue to apply data moves, consider the advantages and disadvantages of the two different methods presented here. Which methods might work best for you and do you imagine using both base R and dplyr for your data moves in R?\n\n\nIn this section we learned about important data moves that represent common programming applications for data dives and explorations within the data science workflow. We realized these data moves using base R and the dplyr package (with help from magrittr). Furthermore, we introduced loops and user-defined functions as examples of how various concepts come together and play a role in the data exploration process. As you move forward with using data moves for your data dive, think about the questions that come to mind via this exploration. Your data moves can even guide you towards a new data hypothesis!\n\n\n\n\n1. Erickson T, Wilkerson M, Finzer W, Reichsman F. Data moves. Technology Innovations in Statistics Education (2019) 12:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project Part 3.1: Diving into Data Exploration - R</span>"
    ]
  },
  {
    "objectID": "proj4P.html",
    "href": "proj4P.html",
    "title": "4  Python - the Basics",
    "section": "",
    "text": "4.1 Options for your Jupyter notebook workspace\nA JupyterLab is an interactive development environment for working with Jupyter notebooks. Having access to a JupyterLab eliminates the need to download, install, and configure Jupyter notebooks on your device. Options for working with Jupyter notebooks outside of a JupyterLab include Anaconda, Visual Studio Code, and other platforms, such as cloud-based Google Colab. Consider your access needs and choose the platform that works best for you!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#lets-talk-about-python",
    "href": "proj4P.html#lets-talk-about-python",
    "title": "4  Python - the Basics",
    "section": "4.2 Let’s talk about Python",
    "text": "4.2 Let’s talk about Python\nPython is a general-purpose programming language, created by Guido van Rossum, first released in 1991. It was designed to have a straightforward, readable syntax for ease of understanding and coding. Currently, python is used across various fields, including web development, data science, and artificial intelligence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#comments-in-python",
    "href": "proj4P.html#comments-in-python",
    "title": "4  Python - the Basics",
    "section": "4.3 Comments in Python",
    "text": "4.3 Comments in Python\n\n\n\n\n\n\nRecall\n\n\n\nComments should be written as complete sentences and used to provide context or explain decisions that cannot be effectively communicated through other means like descriptive variable names or the structure of your code. Comments should clarify why specific coding choices were made, detail the processes used to implement those choices, and explain the underlying concepts or assumptions that support them.\n\n\nAt times, as seen in the code chunk below, it can be helpful to place comments directly above the code they reference. This can add clarity and context to the code that follows, and may be a more readable option when inline comments do not work as well.\n\n# An arithmetic expression that returns a value.\n\n5 + 3\n\n8\n\n\nAnother use of comments, especially in the collaborative coding process, can be to provide guidance to help those you work with understand your project workflow. For example, comments can indicate when code updates are needed or can serve as a point of reference that specifies the next tasks that need to be completed.\n\n\n# TODO: Check for missing values in the data set.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#python-data-types",
    "href": "proj4P.html#python-data-types",
    "title": "4  Python - the Basics",
    "section": "4.4 Python Data Types",
    "text": "4.4 Python Data Types\nPython provides a variety of built-in data types and data structures. We will emphasize the datatypes that are most common, essential for beginners, and that arise in exploratory data analysis settings.\nThese core data types include:\n\n\n\n\n\nData Type\nDescription\n\n\n\n\nInteger (int)\nRepresents whole numbers, e.g., 10 or -5.\n\n\nFloat (float)\nRepresents decimal numbers, e.g., 3.14 or -10.5.\n\n\nString (str)\nRepresents sequences of characters or text, e.g., 'Hello' or 'Bien, y tu'\n\n\nBoolean (bool)\nRepresents logical values, True or False.\n\n\n\n\n\n\n\nPython is also an object-oriented programming language meaning almost everything that you interact with is an object. Basic data types, like a number (e.g., 10) or a string (e.g., “Kon’nichiwa sekai”) are objects that come with built-in capabilities. For example, numbers support arithmetic operations, while strings allow for actions like concatenation used for combining strings together.\nMany objects in Python, like numbers and strings, are instances of built-in classes. A class is essentially a template that defines the structure and behavior of objects. It specifies what attributes, or properties, the objects will have and what methods, or actions, they can perform. The int data type is an example of such a class. It has attributes, such as an associated value (e.g., 10), and methods or actions that can be invoked, like addition (e.g., 10 + 5 returns 15).\n\n4.4.1 Determining a variable type\nTo determine the particular type (or class) of a variable in Python, we can use the aptly named type() function. We can easily imagine uploading a dataset, perhaps with limited data dictionary information, and having the need to understand or verify variable types.\nLet’s look at a few examples below to explore default associations for different forms of data we might encounter.\n\n# Exploring the class of a counting number, like 10\n\ntype(10)\n\n&lt;class 'int'&gt;\n\n\nNow we see that python returns the class int for the number 10. Let’s explore the types of a few other examples of common data points.\n\n# decimal\n\ntype(3.14159)\n\n&lt;class 'float'&gt;\n\n# a phrase\n\ntype(\"Kon'nichiwa sekai\")\n\n&lt;class 'str'&gt;\n\n# a logical value\n\ntype(True)\n\n&lt;class 'bool'&gt;\n\n\nThe output lists the corresponding classes in the order in which the code was executed, from top to bottom. Note that the bool (logical) value True is case sensitive, where the first letter is capitalized.\n\n\n4.4.2 Objects & Operators\nAs noted above, certain classes have certain operators with which they can interact. However, the same operator can perform different actions in different contexts. In this case, the context would be the variable types to which the operator is applied. To understand the multi-functionality of certain operators let’s observe the examples in the following two blocks of code.\n\n# Let's apply `+` to two numbers (`int` + `float`)\n\n10 + 3.14159\n\n13.14159\n\n\nAs we might expect, applying the + operator to two numbers results in their addition. But, what if the values are non-numeric - will this produce an error?\n\n# Let's apply `+` to words \n\n\"in a day,\" + \" or twoooo\"\n\n'in a day, or twoooo'\n\n\nAha! The + operators also works on the class str (e.g., phrases) and results in concatenation.\n\n\n\n\n\n\nKnow your object\n\n\n\nAn important takeaway is that understanding the characteristics of objects and the associated methods is essential to the programming process.\n\n\nNotice, in the code above, we added a space to the concatenation by making this space a part of the second string.\n\n\n4.4.3 Additional arithmetic operators in Python\n\n\n\n\n\nOperator\nDescription\nExample\nResult\n\n\n\n\n`+`\nAddition\n5 + 3\n8\n\n\n`-`\nSubtraction\n5 - 3\n2\n\n\n`*`\nMultiplication\n5 * 3\n15\n\n\n`/`\nDivision\n5 / 3\n1.6667\n\n\n`%`\nModulus (remainder)\n5 % 3\n2\n\n\n`**`\nExponentiation (power)\n5 ** 3\n125",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#variables---revisited",
    "href": "proj4P.html#variables---revisited",
    "title": "4  Python - the Basics",
    "section": "4.5 Variables - revisited",
    "text": "4.5 Variables - revisited\nWith respect to the programming context, in Python a variable is a named reference to an object. Variables can be thought of as labels associated with stored data. These labels can be referenced in your program for viewing or for the purposes of accomplishing other data processing tasks. Just as in R, variables in Python can contain anything from a single value to a complete dataset, or even more.\nVariable names in Python can only include lower and upper case letters, digits from 0-9, and underscores (_). Although digits are allowed within a variable name, the names cannot begin with a digit. Variable names also cannot contain a space or be a reserved word - a word that has a predefined meaning and specific purpose within Python - like True.\nRecall that we strive for clarity and reproducibility in our data science programming processes. So, despite the flexibility we may have with naming a particular object, or variable, it is a good practice to use descriptive variable names when appropriate. Following this practice can improve the readability and comprehension of your program, which are important in general and particularly in collaboration.\n\n# what did these numbers mean again?\n\nx = \"201\"\ny = \"001\"\nz = 1\n\nAbove, we used perhaps convenient, but absolutely uninformative, variable names for three different numbers (although of different types). However, it wouldn’t take much longer to think of informative variable names, as in the code below.\n\n# course information\n\ncourse_code = \"201\"\ncourse_section = \"001\"\nCreditHours = 1\n\nIn addition, a little time upfront can save a lot of time in the long run. For example, if you revisit this code later on, you likely won’t have to wonder what the variable course_code contains, as where you might have to investigate or memorize the contents of a variable labeled x.\nAs you know, informative code comments are great and also essential to our processes. Keep in mind, though, that informative comments alone are not as useful as informative comments and descriptive variable names!\n\n\n\n\n\n\nVariable name conventions\n\n\n\nAs you develop your programming style, and think about future data science collaborations, you may even want to consider a process for naming conventions. Common naming conventions like camelCase, PascalCase, and snake_case are useful choices that can be made to be descriptive and are acceptable across various programming languages, including R and Python (1).\n\n\nIn this chapter, we will follow the naming conventions outlined in PEP 8 (Python Enhancement Proposal 8), which is the official style guide for Python code. For this, snake_case will be used for variables and user-defined functions, and all caps with underscores (ALL_CAPS) will be used for constants.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#assignment-statements---revisited",
    "href": "proj4P.html#assignment-statements---revisited",
    "title": "4  Python - the Basics",
    "section": "4.6 Assignment statements - revisited",
    "text": "4.6 Assignment statements - revisited\nIn R, it is typical to use the &lt;- operator for assignment statements. For assignment statements in Python the = operator is used in all cases for assigning values to variables.\n\n# assignment statement in Python are made with \"=\"\n\n# assign 5 to the variable radius\nradius = 5\n\n# let's create a variable \"diameter\" using \"radius\"\ndiameter = 2 * radius\n\n# call diameter to display it's contents\ndiameter\n\n10\n\n\nAbove we used the assignment operator = to create the variable radius and then used an expression to create a new variable called diameter. We can call our variables simply by referencing them as in the output above that is the result of running the line of code with diameter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#data-structures-in-python",
    "href": "proj4P.html#data-structures-in-python",
    "title": "4  Python - the Basics",
    "section": "4.7 Data structures in Python",
    "text": "4.7 Data structures in Python\nPython has built-in data structures to store collections of data. These data structures include:\n\n\n\n\n\nData.Structure\nDescription\n\n\n\n\nList (`list`)\nAn ordered collection that can store different data types, where elements can be added, modified, or removed. Example: [1, \"two\", 3.14, False].\n\n\nTuple (`tuple`)\nSimilar to a list, but once created, its elements cannot be changed. Example: (1, \"two\", 3.14, False).\n\n\nDictionary (`dict`)\nAn unordered collection of key-value pairs where each key must be unique and values are accessed using their keys. Example: {\"name\": \"Dana\", \"age\": 50}, where the key-value pairs are \"name\" (key) and \"Dana\" (value) and \"age\" (key) and \"50\" (value).\n\n\n\n\n\n\n\n\n4.7.1 Lists\nA list in Python is an ordered collection of items that may be of different types of data, such as numbers, strings, or even other lists. Lists are mutable, meaning you can add, modify, or remove their elements. Each element in a list is stored in a specific position called an index.\n\n\n\n\n\n\nNote\n\n\n\nIn Python the list indices begin at 0! This means that the first element in a list is represented by the 0 index position. The second element in a list is represented by the 1 index positions, and so on. This is a marked difference from indexing in R.\n\n\nBelow, the list [1, \"two\", 3.14, False] contains four elements: an integer (1), a string (\"two\"), a float (3.14), and a Boolean (False). In this case, the first element (1) is represented by the zeroth position (or the 0 index number), the second element (\"two\") is represented by the first position (or 1 index number), and so on. In general, the indices increase by 1 for each subsequent element of a list.\nIn the code below, as assignment statement is used to assign the list [1, \"two\", 3.14, False] to the variable example_list.\n\n# Create a list of the elements 1, \"two\", 3.14, False\n\nexample_list = [1, \"two\", 3.14, False]\n\n# Display the contents of example_list\n\nexample_list\n\n[1, 'two', 3.14, False]\n\n\nWe can access a specific element of example_list by denoting its index position inside of brackets (i.e., using bracket notation). Below we get the first element of example_list by referencing the zeroth index position.\n\n# Access the first element of example_list (in the zeroth index position)\n\nexample_list[0]\n\n1\n\n\nTo view each element in the list individually, we can specify the relevant indices within the list, via bracket notation, as inputs to the print() function.\n\nprint(example_list[0])\n\n1\n\nprint(example_list[1])\n\ntwo\n\nprint(example_list[2])\n\n3.14\n\nprint(example_list[3])\n\nFalse\n\n\nCan we view list elements in sequence without having to type out each one? Of course! One way to do this is to use the slice syntax denoted by a colon :. For example, example_list[1:3], extracts the elements starting from index 1 up to, but not including, index 3.\n\n# Access elements starting from index 1 up to, but not including, index 3. \n\nexample_list[1:3]\n\n['two', 3.14]\n\n\nA slice will return a list even if it is a single element.\n\n# Access elements starting from index 2 up to, but not including, index 3. \n\nexample_list[2:3]\n\n[3.14]\n\n\nTake a moment to consider how you would get the first through third elements of example_list (or a general list).\n\n4.7.1.1 Common list methods\nIn data science, Python lists are often used to store, organize, and manipulate data using built-in list methods. A method is a function that is associated with an object type and is used to perform actions or operations on that object. Methods in Python are called using a specific syntax known as dot notation. In dot notation, the format is object.method(arguments). The method comes after the dot and tells Python what action to perform on the object that precedes it.\nWhile there is a comprehensive list of methods available for lists, we will focus on those most commonly used. In particular, this section will cover methods used for adding, removing, and rearranging elements within a list.\n\n4.7.1.1.1 .append()\nThe .append() method adds a single element to the end of a list. The element can be of any data type such as a number, string, or even another list. This operation is done in place, meaning it modifies the original list directly and does not require an additional assignment statement.\n\n# Create a list of the elements 1, \"two\", 3.14, False\n\nexample_list = [1, \"two\", 3.14, False]\n\n# Add the element 5 to example_list\n\nexample_list.append(5)\n\n# Display the contents of example_list\n\nexample_list\n\n[1, 'two', 3.14, False, 5]\n\n\n\n\n\n4.7.1.2 .remove()\nThe .remove() method removes only the first occurrence of a specified element from a list, even if the element appears multiple times. Like .append(), the .remove() method performs the operation in place.\n\n# Create a list of the elements 1, \"two\", 3.14, False\n\nexample_list = [1, \"two\", 3.14, False]\n\n# Remove the element 'two' in example_list\n\nexample_list.remove(\"two\")\n\n# Display the contents of example_list\n\nexample_list\n\n[1, 3.14, False]\n\n\nThe code below illustrates how only the first object of a repeating list element is removed using .remove().\n\n# A list with \"two times\" repeated \n\nrepeat2x_list = [1, \"two times\", \"two times\", 3]\n\n# Display the contents of repeat2x_list\n\nrepeat2x_list\n\n[1, 'two times', 'two times', 3]\n\n# Remove the element 'two times' in repeat2x_list\n\nrepeat2x_list.remove(\"two times\")\n\n# Display the contents of repeat2x_list after .remove()\n\nrepeat2x_list\n\n[1, 'two times', 3]\n\n\n\n\n4.7.1.3 .sort()\nThe .sort() method arranges the elements of a list. Ascending order is the default arrangement. However, unlike the two prior methods, the .sort() method only works on homogeneous lists. This means that all elements in the list must be of the same data type (e.g., all integers or all strings).\nTo sort a list in descending order, you can use the reverse=True argument within the .sort() method. This will reverse the default sorting behavior, placing the largest elements first.\n\n# A list containing the elements \"elephant\", \"zebra\", \"tiger\", \"panda\"\n\nanimals = [\"elephant\", \"zebra\", \"tiger\", \"panda\"]\n\n# Sort in ascending order\n\nanimals.sort()\n\n# Display the contents \n\nanimals\n\n['elephant', 'panda', 'tiger', 'zebra']\n\n\n\n# A list containing the elements \"mango\", \"apple\", \"cherry\", \"banana\" \n\nfruits = [\"mango\", \"apple\", \"cherry\", \"banana\"]\n\n# Sort in ascending order\n\nfruits.sort(reverse = True)\n\n# Display the contents\n\nfruits\n\n['mango', 'cherry', 'banana', 'apple']\n\n\n\n\n\n4.7.2 Tuple\nIn Python, a tuple is an ordered collection of items that can be of different data types, such as numbers, strings, or even other tuples. Like lists, each element in a tuple is assigned an index, starting at 0 for the first element, 1 for the second, and so on, incrementing by 1 for each subsequent element.\nBelow, we use an assignment statement to assign the tuple (1, “two”, 3.14, False) to the variable example_tuple.\n\n# Create a tuple of the elements 1, \"two\", 3.14, False\n\nexample_tuple = (1, \"two\", 3.14, False)\n\n# Display the content of example_tuple\n\nexample_tuple\n\n(1, 'two', 3.14, False)\n\n\nSo is there a difference between lists and tuples? Well, yes…\nThe main difference between a list and a tuple is that lists are mutable (you can change their elements), while tuples are immutable (their elements cannot be changed once created).\n\n\n4.7.3 Dictionary\nA dictionary in Python is an unordered collection of key-value pairs that can store items of different data types, such as numbers, strings, lists, tuples, or even other dictionaries. Unlike lists or tuples, a dictionary does not use numeric index values. Instead, each element (or value) of a dictionary is accessed using its corresponding, unique key. In addition to being unique, dictionary keys must be immutable data types meaning they cannot be changed after being created. For example, keys can be numbers, strings, or tuples!\n\n\n\n\n\n\nTuples as dictionary keys\n\n\n\nEven though tuples themselves are immutable, they can still contain mutable objects like lists. However, to use a tuple as a key in a dictionary, everything inside the tuple must also be immutable. For example, a tuple that contains a list cannot be used as a dictionary key, because lists can change: But, dictionary keys must stay the same.\n\n\nFor example, the dictionary {1 : \"Turtle Power\", \"id\" : \"Leonardo\", 2 : \"pizza\", \"total\" : 4, \"leader\" : True} contains five key-value pairs: a number key 1 with a value of \"Turtle Power\", a string key \"id\" with a value of \"Leonardo\", a number key 2 with a string value of \"pizza\", a string key \"total\" with a number value of 4, and a string key \"leader\" with a Boolean value of True. Each key is a unique identifier for its corresponding value.\nBelow, we use an assignment statement to assign the dictionary {1 : \"Turtle Power\", \"id\" : \"Leonardo\", 2 : \"pizza\", \"total\" : 4, \"leader\" : True} to the variable tmnt_dict.\n\n# A dictionary with key-value pairs\n# 1-\"Turtle Power\", \"id\"-\"Leonardo\", 2-\"pizza\", \"total\"-4, \"leader\"-True\n\ntmnt_dictionary = {1 : \"Turtle Power\", \"id\" : \"Leonardo\", 2 : \"pizza\", \"total\" : 4, \"leader\" : True}\n\n# Display the contents\n\ntmnt_dictionary\n\n{1: 'Turtle Power', 'id': 'Leonardo', 2: 'pizza', 'total': 4, 'leader': True}\n\n\nSince dictionaries are not ordered the values cannot be accessed using indices. Instead, to access the values in a dictionary we need to use key names with bracket notation. For instance, the expression tmnt_dictionary[“id”] can be used to retrieve the value associated with the key ”id”.\nBelow, we use the print() function, our dictionary reference and the keys in bracket notation to view each of the values.\n\nprint(tmnt_dictionary[1])\n\nTurtle Power\n\nprint(tmnt_dictionary[\"id\"])\n\nLeonardo\n\nprint(tmnt_dictionary[2])\n\npizza\n\nprint(tmnt_dictionary[\"total\"])\n\n4\n\nprint(tmnt_dictionary[\"leader\"])\n\nTrue\n\n\nSince a dictionary is a mutable data structure, we can add new key-value pairs using notation that is similar to how we access values. For example, the assignment statement tmnt_dictionary[(“Type”, “id2”)] = [“Nunchucks”, “Michelangelo”] adds the key-value pair (“Type”, “id2”)-[“Nunchucks”, “Michelangelo”] to tmnt_dictionary, where the key (“Type”, “id2”) is a tuple and the value [“Nunchucks”, “Michelangelo”] is a list.\n\n# Add the key-value pair (\"Type\", \"id2\")-[\"Nunchucks\", \"Michelangelo\"] to tmnt_dictionary\n\ntmnt_dictionary[(\"Type\", \"id2\")] = [\"Nunchucks\", \"Michelangelo\"]\n\n# Display tmnt_dictionary\n\ntmnt_dictionary\n\n{1: 'Turtle Power', 'id': 'Leonardo', 2: 'pizza', 'total': 4, 'leader': True, ('Type', 'id2'): ['Nunchucks', 'Michelangelo']}\n\n\n\n\n4.7.4 Common dictionary methods\nDictionaries are often used in data science to store and organize data. To facilitate the use of dictionaries, Python has a set of methods that can be used to perform actions on these data structures. Next, we’ll look at a few methods used that can be used to add and access key-value pairs within a list.\n\n4.7.4.1 .items()\nThe .items() method is used to retrieve all key-value pairs in a dictionary. It returns a read-only view of the dictionary data called dict_items. In Python, dict_items is a special type of object that shows all the key-value pairs in a dictionary as a collection of tuples. Each tuple contains one key and its corresponding value.\n\n# A dictionary with key-value pairs\n\ntmnt_dictionary = {1: 'Turtle Power', 'id': 'Leonardo', 2: 'pizza', 'total': 4, 'leader': True, ('Type', 'id2'): ['Nunchucks', 'Michelangelo']}\n\n# Display the key-value pairs as tuples using .items()\ntmnt_dictionary.items()\n\ndict_items([(1, 'Turtle Power'), ('id', 'Leonardo'), (2, 'pizza'), ('total', 4), ('leader', True), (('Type', 'id2'), ['Nunchucks', 'Michelangelo'])])\n\n\n\n\n4.7.4.2 .keys()\nThe .keys() method can be used to retrieve all the keys in a dictionary as a dict_items object.\n\n# display the keys of tmnt_dictionary\n\ntmnt_dictionary.keys()\n\ndict_keys([1, 'id', 2, 'total', 'leader', ('Type', 'id2')])\n\n\n\n\n4.7.4.3 .values()\nYou likely can guess what the .values() method returns. If you were thinking that this method will return all of the values in a dictionary as a dict_items object, you’d be right!\n\n# display the values of tmnt_dictionary\n\ntmnt_dictionary.values()\n\ndict_values(['Turtle Power', 'Leonardo', 'pizza', 4, True, ['Nunchucks', 'Michelangelo']])\n\n\nWithin a data science workflow, lists, tuples, and dictionaries can serve different purposes and meet different needs. In this way, each of these Python data structures are useful and important. For comparisons, lists are often better for storing and modifying collections of data as where tuples may be preferred for fixed data like names or IDs. Dictionaries would likely be preferable for organizing data that needs to be accessed using reference labels or keys.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj4P.html#python-libraries",
    "href": "proj4P.html#python-libraries",
    "title": "4  Python - the Basics",
    "section": "4.8 Python libraries",
    "text": "4.8 Python libraries\nA library in Python is similar to a package in R. A Python library is a collection of pre-written code that provides functions, classes, and modules to perform tasks like data manipulation, analysis, and visualization. Python offers several libraries that are commonly used in data science.\n\n4.8.1 NumPy\nNumPy (Numerical Python) is a library that facilitates numerical data processing. This library includes specialized data structures, such as multidimensional arrays, that allow for vectorized operations. Recall that vectorization allows for element-wise computations without the need for loops. NumPy also has built-in functions for fast and efficient mathematical and statistical calculations.\nBefore we can use NumPy features, we have to first import the library into a script or Jupyter Notebook. We can do this for a general Python library by using the import command followed by the library name. Some libraries, like NumPy, are commonly imported with an alias to simplify the code syntax when accessing features of the library. For example, import numpy as np allows you to use np as a shorthand when calling NumPy functions.\n\n## Import the numpy library with the alias np\nimport numpy as np\n\nThe standard convention for calling NumPy functions is to prefix them with np.\n\n# Store the mean of the list into ex_calc\n\nex_calc = np.mean([1, 2, 3, 4, 5])\n\n# display the value of ex_calc using print()\n\nprint(ex_calc)\n\n3.0\n\n\nNumPy functions, like np.mean(), are primarily designed to work with NumPy arrays. However, many of them also accept Python lists as input because NumPy automatically converts lists into arrays internally. This was the case in the code above since [1,2,3,4,5] is a list. Although this conversion worked in this case, using arrays directly is better to ensure proper compatibility and performance.\n\n\n4.8.2 pandas\nR includes many data analysis features in its base version. This is not the case for base Python. So, in Python, we need to leverage libraries like pandas to get the specialized functions and tools we need for our data science tasks. So what is pandas?\npandas is a Python library used for data analysis and data processing. It provides data structures like Series and DataFrames, which can be used to store, organize, manipulate, and analyze data. pandas provides tools to facilitate data cleaning, data moves, like filtering & merging, data visualization and more.\nAs we did with NumPy, we can import pandas with the import statement and create an alias for it.\n\n## Import the pandas library with the alias pd \n\nimport pandas as pd\n\nAfter importing the pandas library, we gain access to data structures and tools essential for performing various data moves.\n\n4.8.2.1 Series and DataFrames\nThe main data structures that we will use are the Series and the DataFrame.\nA Series is a 1-dimensional labeled array that is similar to a vector in R. Each element has the same data type and is associated with an index, which serves as the label for accessing the data.\nA DataFrame in pandas is a 2-dimensional labeled data structure that organizes data in a tabular format (i.e., rows and columns). A DataFrame can contain multiple data types because each column is a Series with its own data type. Columns are labeled (typically by name), and rows are identified by an index. A DataFrame is like a spreadsheet, where each row represents an observational unit and each column represents a characteristic or feature of that observed unit.\n\n\n4.8.2.2 Pandas Functions\nPandas provides a comprehensive set of attributes and functions that can be used to analyze and visualize data. Attributes are properties of pandas objects that provide information about the object. For example, an attribute can be used to get the dimensions of a DataFrame.\nFunctions (also called methods) perform operations on pandas objects. We might use a function to compute the mean of the values in a numerical Series, for example. By using pandas attributes and functions, we can perform data moves to facilitate analysis and prepare data for visualization!\n\n\n\n\n1. Pruim R, Gı̂rjău M-C, Horton NJ. Fostering better coding practices for data scientists. Harvard Data Science Review (2023) 5:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python - the Basics</span>"
    ]
  },
  {
    "objectID": "proj5P.html",
    "href": "proj5P.html",
    "title": "5  Project Part 3.2: Diving into Data Exploration - Python",
    "section": "",
    "text": "5.1 Data Moves in Python\nThe Ramen Rater has been around since 2002. The website’s founder, Hans Lienesch, reviews ramen noodles and records data on the brand, variety, style, country, and his own personal rating with scores given in increments of 0.25. As of now, the list contains reviews dating back to 2002. To expand upon the background description and address other components of the data dictionary criteria, we will read in the The Big List dataset and use some functions to access general metadata and information on the variables within.\n## Load in the data into a pandas DataFrame, using the pd.read_csv function\nimport pandas as pd\n\nramen = pd.read_csv(\"theramenrater_big_list.csv\")\nWhen data is read in through the pandas read_csv method the default structure is a DataFrame. Recall, that DataFrames store data in tabular format (e.g., like rows and columns in a spreadsheet).\nWe could use pandas functions (or methods) to extract useful information about the data, such as column names and data types. This information is useful in general and is often an initial, default, step in a data science workflow upon reading in data. We can begin with using the .info() method which will return a summary of information about the structure of the DataFrame.\n## List summary information about the ramen dataframe\n\nramen.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5015 entries, 0 to 5014\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   review_number  5015 non-null   int64  \n 1   brand          5015 non-null   object \n 2   variety        5015 non-null   object \n 3   style          5015 non-null   object \n 4   country        5015 non-null   object \n 5   stars          5015 non-null   float64\ndtypes: float64(1), int64(1), object(4)\nmemory usage: 235.2+ KB\nThe output above shows that there are 5015 entries (rows) and 6 columns. In the table, each row represents a column in the ramen dataset and # shows the index for the columns (from 0 and to 5), the column name, the Non-Null Count (i.e., the number of non-missing values), and the data type. The review_number column is int64 (numerical), the stars column is float64 (numerical), and the remaining columns are objects. When a column’s data type is listed as object, the values could be strings, numbers, or a mix of both. All of this information is useful and includes things we would want to include in something like a data dictionary.\nWe could also retrieve individual pieces of this information using DataFrame attributes like .shape and .columns. Attributes are properties of pandas objects, such as metadata (e.g., column names, indices), that can be accessed using dot notation just like methods. Unlike methods, though, attributes are called without using parentheses ().\n## Check the dimensions of the dataset\n\nramen.shape\n\n(5015, 6)\n## List the variables in the dataset\n\nramen.columns\n\nIndex(['review_number', 'brand', 'variety', 'style', 'country', 'stars'], dtype='object')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Part 3.2: Diving into Data Exploration - Python</span>"
    ]
  },
  {
    "objectID": "proj5P.html#data-moves-in-python",
    "href": "proj5P.html#data-moves-in-python",
    "title": "5  Project Part 3.2: Diving into Data Exploration - Python",
    "section": "",
    "text": "5.1.1 Accessing Column Values\nTo retrieve all values from a column in Pandas we can use bracket notation where the column name is placed inside of square brackets and enclosed by single or double quotes. For example, to access the values in the style column we can use ramen[‘style’]. The returned object is a Series.\n\nramen['style']\n\n0       Pack\n1       Bowl\n2       Bowl\n3       Bowl\n4        Cup\n        ... \n5010    Pack\n5011    Pack\n5012    Pack\n5013    Pack\n5014     Cup\nName: style, Length: 5015, dtype: object\n\n\nA benefit of returning a Series is that it supports vectorized operations. Pandas also provides built-in methods for Series that perform a wide range of tasks for both categorical and numerical data. In addition, Series have attributes that store information such as the data type and size.\n\n\n5.1.2 Summarizing\nThe summarizing data move can be used to condense the contents of a variable into frequency counts or proportions for categorical variables, and other descriptive statistics of interest for numerical variables. As an example, we can use the .value_counts() method to tally the different styles of noodles.\n\n## Create a frequency table of the labels in the 'styles' column\n\nramen['style'].value_counts()\n\nstyle\nPack          2637\nBowl          1022\nCup           1002\nTray           228\nBox            120\nRestaurant       3\nBar              1\nBottle           1\nCan              1\nName: count, dtype: int64\n\n\nIt appears that most of the reviewed noodles are sold in packs, bowls, or cups. Slightly more than half were in packs, while bowls and cups make up another 40%.\nWe can use the .describe() method to get a comprehensive summary of a numerical variable. The describe() method returns a summary that includes the number of non-missing values, as well as statistical measures that include the mean, standard deviation, minimum, 25th percentile (Q1), 50th percentile (median), 75th percentile (Q3), and maximum.\n\n## Generate summary statistics for the 'stars' column\n\nramen['stars'].describe()\n\ncount    5015.000000\nmean        3.735278\nstd         1.090282\nmin         0.000000\n25%         3.250000\n50%         3.750000\n75%         4.500000\nmax         5.000000\nName: stars, dtype: float64\n\n\nThe output above allows us to get an idea of the distribution of the data. It appears the ratings may be slightly skewed to the right when observing the differences between Q2 and the median vs. the median and Q3. Looking ahead, a related visualization could provide an informative visual depiction of the data distribution.\n\n\n5.1.3 Filtering\nRecall that the filtering data move is what we use to reduce or examine a dataset based on certain row criteria. In our ramen dataset we have the country of origin for each noodle and it may be of interest to analyze the information related to a specific country. For example, suppose we wanted to filter for all the noodles that are manufactured in Japan. How can we do this?\n\n5.1.3.1 Boolean Masks\nA Boolean mask is typically a Series containing True and False values. It is created by applying comparison operators (==, &gt;, &lt;, etc.) or logical conditions (&, |, etc.). To create a Boolean mask for the rows where the country is Japan, we can use the comparison statement ramen[‘country’] == “Japan”. This returns a Series that can be used to filter out, or hide, the rows we do not want (e.g., countries other than Japan).\n\n## Create a Boolean mask where 'country' == \"Japan\"\n\nramen['country'] == \"Japan\"\n\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n5010    False\n5011    False\n5012    False\n5013    False\n5014    False\nName: country, Length: 5015, dtype: bool\n\n\nSo, now that we have boolean masks at our disposal, we can use one to filter the data of interest. To do this, we can assign the filter criteria to a variable (perhaps named mask) and use it to filter the dataframe.\n\n## Create the Boolean mask where 'country' == \"Japan\"\n\nmask = ramen['country'] == \"Japan\"\n\n## Filter the dataframe \n## The .head() method limits the output to the first 5 rows\n\nramen[mask].head()\n\n      review_number     brand                    variety style country  stars\n1105           3464  Takamori              Agodashi Udon  Pack   Japan    5.0\n1106           4613    Ippudo       Akamaru Modern Ramen   Box   Japan    5.0\n1107           3382    Itsuki              Akikara Ramen  Pack   Japan    5.0\n1108           4308   Marutai  Asahikawa Soy Sauce Ramen  Pack   Japan    5.0\n1109           2974    Itomen               Bansyu Ramen  Pack   Japan    5.0\n\n\nNote: The use of the .head() method returns only the first five rows (from the filtered dataframe). What method could we use to see that the filtered data contains 1039 entries?\nNext, just in case the opportunity to travel to Japan arises and so as to lean on data for the best ramen experience, let’s find out about the ramen that’s rated the best!\n\n## Find the average 'stars' rating\n## The .mean() method calculates the mean of a numerical Series \nmean_stars = ramen['stars'].mean()\n\n## Create the Boolean mask where 'country' == \"Japan\" and 'stars' greater\n## than or equal to the average stars rating\nmask = (ramen['country'] == \"Japan\") & (ramen['stars'] &gt;= mean_stars)\n\n## Filter the dataframe and display the number of rows that match the condition\nramen[mask].shape[0]\n\n660\n\n\nIt looks like we have 660 options to choose from, so maybe we should narrow the search. But first, let’s see what’s going on in the code above.\n\nWe took the mean of the ratings and stored it in mean_stars.\nThen, added more criteria to our mask variable to filter rows where the country is Japan and (&) the ratings are greater than or equal to (&gt;=) the overall average.\nLastly, we applied the mask criteria to our ramen data and called the shape attribute (which gives the dimensions).\n\nNOTE: Since Python indices begin at zero, the [0] index of shape yields the number of rows.\n\n\n\n\n\n\n\n\nSo, what about ramen?\n\n\n\nAlthough ramen was originally invented in China, it was Japan that refined and popularized it, and as a result, modern ramen is distinctly Japanese. So much so that it is featured in the Shin-Yokohama Ramen Museum, which documents its rich history and tasty transformations over time!\nNote, that contextual information can facilitate your data exploration workflow and efficacy.\n\n\n\n\n5.1.3.2 Query\nIn programming, a query is a request to access or retrieve information from a database or dataset based on conditions or criteria. Queries may also include instructions to filter, sort, or aggregate the data.\nIn Pandas, the .query() method is specifically used to filter rows in a DataFrame based on a string input that consists of a boolean expression. For example, we can assign a string that contain a boolean expression (i.e., filtering criteria) to a variable and pass it as the parameter to .query().\nLet’s use this method to show a different way to filter the ramen DataFrame for rows where the country is equal to Japan.\n\n## Define a query string to filter the DataFrame for rows where the 'country' is \"Japan\" \n\nq = \"country == 'Japan'\"\n\n## Use the .query() method to filter the DataFrame and access the first 5 entries (with the .head() method)\n\nramen.query(q).head()\n\n      review_number     brand                    variety style country  stars\n1105           3464  Takamori              Agodashi Udon  Pack   Japan    5.0\n1106           4613    Ippudo       Akamaru Modern Ramen   Box   Japan    5.0\n1107           3382    Itsuki              Akikara Ramen  Pack   Japan    5.0\n1108           4308   Marutai  Asahikawa Soy Sauce Ramen  Pack   Japan    5.0\n1109           2974    Itomen               Bansyu Ramen  Pack   Japan    5.0\n\n\nNotice that the results are identical to those obtained using a Boolean mask. While the advantages of using .query() may not be clear for a single condition, its benefits become more apparent with more complex filtering. For example, suppose we want to find extreme cases where the noodle ratings are 0.0 or 5.0, for Myojo brand, Tray-style noodles, manufactured in Japan.\n\n## Define a query string to filter the DataFrame for \n## rows where the 'country' is \"Japan\",  \n## the style of the noodles is \"Tray\",\n## the brand is \"Myojo\", and\n## the stars rating is either 0.0 or 5.0 \n\nq = 'country == \"Japan\" and style == \"Tray\" and brand == \"Myojo\" and (stars == 0.0 or stars == 5.0)'\n\n## Use the .query() method to filter the DataFrame with criteria q \nramen.query(q)\n\n      review_number  brand  ... country stars\n1112           3940  Myojo  ...   Japan   5.0\n1151           2951  Myojo  ...   Japan   5.0\n1170           4686  Myojo  ...   Japan   5.0\n1171           1103  Myojo  ...   Japan   5.0\n1172           2801  Myojo  ...   Japan   5.0\n1173           5014  Myojo  ...   Japan   5.0\n1174           3470  Myojo  ...   Japan   5.0\n1175           3231  Myojo  ...   Japan   5.0\n1270           2906  Myojo  ...   Japan   5.0\n2142           3505  Myojo  ...   Japan   0.0\n\n[10 rows x 6 columns]\n\n\nThe output shows partial information for the first ten rows of the filtered dataset of interest. In terms of the code, in this instance, the query string improves readability and reduces the required syntax in comparison to specifying the same criteria with a Boolean masks. When using a Boolean mask, you need to repeatedly reference the DataFrame and use bracket notation. In contrast, .query() lets you reference column names directly as variables in a string expression, which is often more efficient.\n\n\n\n5.1.4 Subsetting\nRecall that the subsetting data move involves reducing the data based on column criteria. We can subset a Pandas DataFrame by specifying columns to keep using bracket notation, or by dropping columns that are not needed using the .drop() method. For bracket notation, we can input a list object consisting of column names inside brackets as shown below. Subsetting the data frame in this way results in a new DataFrame with the columns of interest.\n\n## List of columns to select\n\ncolumns = ['brand', 'style', 'stars']\n\n## Subset the ramen dataframe\n## ramen[['brand', 'style', 'stars']].head() results in the same\n\nramen[columns].head()\n\n    brand style  stars\n0   Maggi  Pack   5.00\n1  Suimin  Bowl   5.00\n2  Suimin  Bowl   5.00\n3  Suimin  Bowl   5.00\n4  Suimin   Cup   4.25\n\n\nNow, let’s subset by dropping unneeded columns, using .drop().\n\n## Drop the review_number column from the ramen dataframe\n\nramen = ramen.drop(columns = ['review_number'])\n\n## Verify the column was removed\n\nramen.columns\n\nIndex(['brand', 'variety', 'style', 'country', 'stars'], dtype='object')\n\n\n\n\n5.1.5 Grouping\nRecall that the grouping data move results in certain categorizations that can allow for realted comparisons via statistics, visualizations, models and more. In the Pandas library, the .groupby() methods groups rows based on values in one or more variables. For example, we may be interested in groupings based on levels within a specified categorical variable (e.g., ramen varieties). Once the grouping has been implemented we can apply aggregation methods, such as summing the totals, for variety.\n\n\n## Create a groupby object\n\nstyle_grps = ramen.groupby('style')\n\nAbove, we created a variable style_grps. This is a grouped object that contains DataFrames organized into subgroups based on the levels within the style column. Each subgroup contains a separate DataFrame consisting of row information corresponding to one of the unique levels (e.g., “Cup”, “Can”, etc.) in style.\nIn the code below, the command style_grps.groups.keys() displays all the subgroup names (i.e., the grouping criteria or the levels of style) in the style_grps object. Note, that the .groups attribute of a grouped object is a dictionary on which we can run the .keys() method.\n\n## Displays all the subgroup names in grps \n\nstyle_grps.groups.keys()\n\ndict_keys(['Bar', 'Bottle', 'Bowl', 'Box', 'Can', 'Cup', 'Pack', 'Restaurant', 'Tray'])\n\n\nVarious data moves are related and can be complimentary. Although the grouping data move does not remove row information (but rather partitions the rows) and filtering does remove certain rows, we could choose to use either of these data moves to accomplish certain desired tasks. For example, if we were interested in comparing the average style rating across each style, we could use our filtering data move for each of the 9 different styles and compute the means of the stars ratings for each filtered dataset. Or, we could leverage grouping to compute the means in a more efficient way using our style_grps object. One difference in using grouping vs. filtering besides reduction in code is that computations applied to the grouped object are vectorized (and also more efficient in that way). The mean stars per group calculation is demonstrated below.\n\n## Calculate the mean stars rating for each style in the style_grps grouped object \n## The .sort_values method arranges the output in ascending order\n\nstyle_grps['stars'].mean().sort_values()\n\nstyle\nCup           3.493693\nCan           3.500000\nRestaurant    3.583333\nTray          3.595965\nBowl          3.664540\nPack          3.851422\nBottle        4.000000\nBox           4.060417\nBar           5.000000\nName: stars, dtype: float64\n\n\nApplying the mean() function to our grouped object results in a Series. We can then apply the .sort_values() method to this Series so that the group means appear in ascending (or descending) order.\nFrom the output we can see that the three highest mean ratings are for Bar, Box and Bottle. To complement the mean rating analysis, we should also examine the number of occurrences of each style, and we can do this by applying the appropriate method to our same style_grps object.\n\n## Return the counts of non-missing values in the stars column \n## for each style in the style_grps object   \n## The .sort_values() method arranges the output in ascending order   \n\nstyle_grps['stars'].size().sort_values() \n\nstyle\nBar              1\nBottle           1\nCan              1\nRestaurant       3\nBox            120\nTray           228\nCup           1002\nBowl          1022\nPack          2637\nName: stars, dtype: int64\n\n\nFrom the output we can see that two of our top three mean ratings are only represented by 1 review. So, we can regard the other averages (besides restaurant) represented by many more ratings as more reliable with respect to the style categorization. The frequencies may reflect the preferences of the rater, or may be reflective of a concept like “availability”. At least, we have an idea about a line of inquiry that may be worth investigating and we might want to put more or less faith in some mean ratings than others. ### Calculating (a new attribute)\nThe calculating a new attribute data move allows us to create new variables from existing ones. We might calculate a new attribute to reveal underlying patterns in the data, to facilitate comparisons, or to preparing data for analysis, among other useful transformations. We can calculate various types of new attributes including numerical or categorical variables. For example, two numerical variables might be combined in a formula to calculate a new quantity, or used to sort observations into groups like small, medium, large. These calculated values become part of the dataset and can be used to support further exploration.\n\n5.1.5.1 A calculating data move example\nThe American Federation of Labor and Congress of Industrial Organizations (AFL-CIO) website provides data on CEO compensation packages in the United States. The information on the compensation packages includes base salary, bonuses, stock awards, and other earnings, all captured in a dataset compiled from AFL-CIO data.\nBelow are the attributes and descriptions for this dataset:\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Create the data frame\ncompensation_table &lt;- data.frame(\n  Name = c( \"ticker\", \"salary\", \"bonus\",\n            \"stock_awards\", \"option_awards\",\"non_equity_comp\",\n            \"pension_change\",\"other_comp\"\n  ),\n  Description = c( \"Stock ticker symbol for the company.\", \n                   \"Base annual salary.\", \"Additional cash bonus.\",\n                   \"Value of stock granted.\", \"Value of stock options granted.\",\n                   \"Performance-based cash compensation not tied to equity.\", \n                   \"Increase in pension value and deferred compensation earnings.\",\n                   \"Miscellaneous compensation (e.g., perks, benefits).\"\n  )\n)\n\nkable(compensation_table, \n      caption = \"CEO Compensation Packages - Variable Descriptions (AFL-CIO)\",\n      booktabs = TRUE,\n      longtable = TRUE,\n      align = \"l\") %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"hold_position\"))\n\n\nCEO Compensation Packages - Variable Descriptions (AFL-CIO)\n\n\nName\nDescription\n\n\n\n\nticker\nStock ticker symbol for the company.\n\n\nsalary\nBase annual salary.\n\n\nbonus\nAdditional cash bonus.\n\n\nstock_awards\nValue of stock granted.\n\n\noption_awards\nValue of stock options granted.\n\n\nnon_equity_comp\nPerformance-based cash compensation not tied to equity.\n\n\npension_change\nIncrease in pension value and deferred compensation earnings.\n\n\nother_comp\nMiscellaneous compensation (e.g., perks, benefits).\n\n\n\n\n\n\n\nAlthough the dataset does not contain a column representing total compensation, we can calculate this attribute from the existing data. We could also create additional attributes that represent various measures like the percentage of total compensation coming from stock or bonus pay.\n\n# Load the CEO compensation data\nceo_pay = pd.read_csv(\"ceo_compensation_summary.csv\")\n\n# Calculate total compensation and store the result in the dataframe in a new column called total   \nceo_pay['total'] = (\n    ceo_pay['salary'] +\n    ceo_pay['bonus'] +\n    ceo_pay['stock_awards'] +\n    ceo_pay['option_awards'] +\n    ceo_pay['non_equity_comp'] +\n    ceo_pay['pension_change']\n)\n\nNow that we’ve computed this new column, additional analysis and exploration can be performed to better understand CEOs payment trends. In addition, we could use the summarizing data move to calculate statistics such as the mean, median, and standard deviation of the new total attribute.\n\n\n\n5.1.6 A little extra - Merging & Joining\nMerging and joining involve combining information from various datasets based on some identifying criteria. These terms are often used interchangeably.\nConsider the ceo_pay dataset, which includes details about the CEO compensation structure but lacks information about the companies and the particular CEOs. If we had the additional company and CEO information in another dataset and a common identifier between these disparate sources, we could merge the data into a single source containing all of the information.\nAs it turns out, the company dataset, read in below, has information on company and ceo names. In addition, both the company and ceo_pay datasets contain a common identifier, ticker. So, we can use this identifier to merge the two data sources into one.\n\n# Load the company information data \n\ncompany = pd.read_csv(\"company_info.csv\")\n\n# Join the CEO pay dataset with the company dataset \n\nceos_and_pay = pd.merge(ceo_pay, company, on = 'ticker', how = 'inner')\n\n# Show the new combined data column names\n\nceos_and_pay.columns\n\nIndex(['ticker', 'salary', 'bonus', 'stock_awards', 'option_awards',\n       'non_equity_comp', 'pension_change', 'other_comp', 'total', 'city',\n       'state', 'display_name', 'fiscal_year', 'ceo_name'],\n      dtype='object')\n\n# preview the first 5 rows\n\nceos_and_pay.head()\n\n  ticker   salary  ...  fiscal_year                      ceo_name\n0    TPG   509615  ...       2023.0           Mr. Jon  Winkelried\n1     CG   838462  ...       2023.0  Mr. Harvey Mitchell Schwartz\n2   AVGO  1200000  ...       2024.0               Mr. Hock E. Tan\n3   PANW   750000  ...       2024.0      Mr. Nikesh  Arora C.F.A.\n4   COTY  3549000  ...       2024.0               Ms. Sue Y. Nabi\n\n[5 rows x 14 columns]\n\n\nAbove we used the pd.merge() function (in the Pandas library) to combine the two dataframes based on the ticker variable/identifier in both datasets. We specified the identifier ticker as the on parameter value, and we specified inner as the how parameter value. The inner specification defines an inner-join which means only the rows where both datasets have a matching ticker value are merged and kept in the resulting dataset. In general pd.merge() is used to combine DataFrames based on one or more common keys (i.e., columns).\nNow that we have additional information in our ceos_and_pay dataset we have the option to explore entirely new data investigations, such as CEO salaries by industry, or salaries by other company characteristics. And of course, to do this, we can leverage data moves!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Part 3.2: Diving into Data Exploration - Python</span>"
    ]
  },
  {
    "objectID": "proj6P.html",
    "href": "proj6P.html",
    "title": "6  Project Part 4.1: Visualizations & Trends in Python",
    "section": "",
    "text": "6.1 A few guidelines\nTo understand features of the data we can start by selecting a visualization that is appropriate for a particular data type. For example, if we’re attempting to gain insight into a single variable consisting of categorical data, we might select a bar chart. This choice of visualization could allow us to compare the frequencies of the various factor levels. For a numerical variable, a histogram could provide similar information and depict relative frequencies or, more generally, the distribution of data values. To gain insights into bivariate relationships, we might consider mosaic plots for two categorical variables, or scatterplots for two quantitative variables. We could even extend the information depicted in a 2-dimensional visualization to multiple variables by including color, facets, and other overlays. Other visualizations that may be informative in depicting trends are line charts, which can show patterns over time (e.g., a time series), or perhaps a spaghetti plot to visualize grouped trends over time. For determining a graphical representation of data, consider: “What visualization can I choose to clearly emphasize trends and patterns while avoiding clutter and unnecessary complexity.”\nHere are a few guidelines:\nYour visualization should be self explanatory to a viewer. This means that there should not be a need to explain all of the details of the visual beyond the information conveyed by the visualization itself.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project Part 4.1: Visualizations & Trends in Python</span>"
    ]
  },
  {
    "objectID": "proj6P.html#a-few-guidelines",
    "href": "proj6P.html#a-few-guidelines",
    "title": "6  Project Part 4.1: Visualizations & Trends in Python",
    "section": "",
    "text": "minimize clutter\nuse appropriate scales and dimensions\nclearly label axes\nconsider order when appropriate (e.g., ordinal information)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project Part 4.1: Visualizations & Trends in Python</span>"
    ]
  },
  {
    "objectID": "proj6P.html#visualizations-in-python",
    "href": "proj6P.html#visualizations-in-python",
    "title": "6  Project Part 4.1: Visualizations & Trends in Python",
    "section": "6.2 Visualizations in python",
    "text": "6.2 Visualizations in python\nAlthough R has excellent visualization capabilities, such as those available through the ggplot2 library, visualizations in Python can be embedded into a larger data science workflow when Python is the preferred language and switching languages or platforms is not desirable. However, utilizing the best of both languages is always an option, especially in platforms that can accommodate multiple languages, such as Quarto, Jupyter Notebook, VSCode, Google Colab and others. In this chapter, we focus on visualizations in python.\n\n6.2.1 Common libraries for python visualizations\nBelow, we will preview two common python visualization libraries and learn how to leverage these methods to learn about our data and investigate trends within.\nmatplotlib\nmatplotlib is a Python library that provides a framework for generating and customizing visualizations. It supports various plot types, including line plots, scatter plots, bar charts, histograms, and more. matplotlib integrates well with other libraries, such as pandas and supports various output formats, including PNG, PDF, and SVG.\nmatplotlib.pyplot\nThe primary module from matplotlib that we’ll use is pyplot. To access pyplot functionality, we need to import both the matplotlib library and the pyplot module.\n\nimport matplotlib.pyplot as plt\n\nIn the code above, we used dot notation where matplotlib is the library, pyplot is the module, and plt is the alias.\npandas\npandas provides built-in plotting functionality that allows you to generate visualizations directly from Series and DataFrames. It uses matplotlib.pyplot for plotting through the .plot() method. This offers a simple way to create basic plots, which can be further customized using matplotlib attributes such as plt.title(), plt.xlabel(), and plt.ylabel().\n\nimport pandas as pd",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project Part 4.1: Visualizations & Trends in Python</span>"
    ]
  },
  {
    "objectID": "proj6P.html#chart-types",
    "href": "proj6P.html#chart-types",
    "title": "6  Project Part 4.1: Visualizations & Trends in Python",
    "section": "6.3 Chart Types",
    "text": "6.3 Chart Types\nThe updated Star Wars dataset comes from the webpage Introduction to Data Analysis Using the Star Wars Dataset by Fabricio Batista Narcizo. This dataset includes information on 87 characters from the first seven episodes of the Star Wars movie saga (the Original Trilogy, the Prequel Trilogy, and The Force Awakens). The author documents the process used to update the Star Wars dataset with additional information from the Star Wars API (SWAPI).\nThe Star Wars dataset contains both categorical and numerical variables that describe each character. We will use this data to demonstrate data visualization in Python, specifically using the matplotlib.pyplot module and the .plot() function from pandas.\nWe will import Pandas and Matplotlib, load the Star Wars dataset, review the variables within, and create related visualizations.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nstarwars = pd.read_csv(\"updated_starwars.csv\")\nstarwars.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 87 entries, 0 to 86\nData columns (total 25 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   name         87 non-null     object \n 1   height       86 non-null     float64\n 2   mass         65 non-null     float64\n 3   hair_color   82 non-null     object \n 4   skin_color   86 non-null     object \n 5   eye_color    87 non-null     object \n 6   birth_year   50 non-null     float64\n 7   birth_era    50 non-null     object \n 8   birth_place  37 non-null     object \n 9   death_year   62 non-null     float64\n 10  death_era    62 non-null     object \n 11  death_place  57 non-null     object \n 12  sex          87 non-null     object \n 13  gender       87 non-null     object \n 14  pronoun      87 non-null     object \n 15  homeworld    83 non-null     object \n 16  species      87 non-null     object \n 17  occupation   87 non-null     object \n 18  cybernetics  7 non-null      object \n 19  abilities    55 non-null     object \n 20  equipment    62 non-null     object \n 21  films        87 non-null     object \n 22  vehicles     15 non-null     object \n 23  starships    20 non-null     object \n 24  photo        87 non-null     object \ndtypes: float64(4), object(21)\nmemory usage: 17.1+ KB\n\n\n\n6.3.1 Bar Plots\nA univariate bar plot is a data visualization used to represent categorical data frequencies. The height or length of each bar (and the associated axis or bar labels) typically corresponds to the level frequencies. Bar plots can be used to compare categories and more generally to visualize a categorical distribution.\n\n6.3.1.1 A (Star Wars) Example\nEven though Star Wars took place in a galaxy far, far away, spanning multiple planets, space stations, and star systems, many of the main characters were human or droid.\n\n# Count and display the number of characters for each species in the dataset. \n\nstarwars['species'].value_counts()\n\nspecies\nHuman               38\nDroid                6\nGungan               3\nMirialan             2\nKaminoan             2\nTwi'lek              2\nWookiee              2\nTrandoshan           1\nYoda's species       1\nHutt                 1\nRodian               1\nNeimodian            1\nToydarian            1\nSullustan            1\nMon Calamari         1\nZabrak               1\nAleena               1\nVulptereen           1\nXexto                1\nToong                1\nCerean               1\nDug                  1\nEwok                 1\nIridonian Zabrak     1\nNautolan             1\nQuermian             1\nTholothian           1\nKel Dor              1\nChagrian             1\nGeonosian            1\nIktotchi             1\nClawdite             1\nBesalisk             1\nSkakoan              1\nMuun                 1\nTogruta              1\nKaleesh              1\nUmbaran              1\nPau'an               1\nName: count, dtype: int64\n\n\nAs seen from the output, .value_counts() returned the various species counts. The returned object is a Series with unique levels (species) as the index and the corresponding counts as values, sorted in descending order of frequency.\nA bar plot with all 39 species represented, would likely break the “minimize clutter” rule, eventhough all of the related information may be important. In particular representing the levels with only one representative in the chart would not necessarily be the best representation. As an alternative, we could use data moves to create a new “other” category that contains the species with fewer than a cutoff count. Or, to represent that a majority of characters come from a few specific species, we can simplify the plot by displaying only the top three frequency categories. Below, we take the second approach.\n\nstarwars['species'].value_counts()[0:3].plot(kind = 'bar', rot = 0)\n\n\n\n\n\n\n\n\nIn the code above, .value_counts() tabulates the occurrences of each species in descending order of frequency, [0:3] filters for the top three entries, and .plot(kind = 'bar') generates the bar plot. The rot = 0 parameter is an option that displays the axis labels horizontally.\nObserving the graph and considering our filtering criteria, we now have a visual representation that shows that the Star Wars character species distribution is human-centric, despite the countless planets across a vast galaxy in which the saga unfolds.\n\n\n\n6.3.2 Histograms\nA histogram is similar to a barplot, but depicts frequencies of binned numerical values vs raw categorical counts. In a histogram, the number of bins is a parameter that impacts the shape of the visualization, as where in a bar plot the number of bars usually corresponds to the number of categorical levels (barring transformation like the filtering we did above). Another common difference between univariate barplots and histograms is that histograms can be displayed using different vertical-axis scales, such as counts, relative frequencies, or densities. When shown on a density scale, the areas of the bars sum to 1 like a probability density function. In contrast, barplots typically display frequencies or proportions for categorical variables, with no regard to bin width or area.\nHistograms can reveal characteristics of the data distribution, including patterns such as modality, shape, skewness, unusual observations and other features that may yield insight into the underlying properties of the variable under consideration.\nSo now that we know about histograms, let’s create one to get a better sense of the distributions of the characters’ heights.\n\nstarwars['height'].plot(kind = 'hist')\n\n\n\n\n\n\n\n\nThe histogram captures most heights between approximately 160 and 200 centimeters. Additionally, the distribution appears to be unimodal and has a slight left skew where more extreme heights are for shorter characters.\n\n# histogram w modified number of bins\n# adding edgecolor helsp distinguish bins\n\nstarwars['height'].plot(kind = 'hist', bins = 20, edgecolor = 'white')\n\n\n\n\n\n\n\n\nIn the code above, we’ve adjusted the default bins parameter, which impacts the bin widths. Correspondingly the number of observations falling within a bin have decreased. In general, the choice of bins can impact the visualization and so interpretations can be subjective in this way. Thus, a histogram can be particularly useful in an exploratory phase of a data science workflow but may not be the best tool on which to base formal or inferential decisions. Nonetheless, in the modified histogram we see a more granular view of the distribution of the Star Wars characters’ heights.\n\n\n6.3.3 Boxplots\nA boxplot is a visual representation of the “five number summary.” Boxplots depict the distribution of numerical data using the five values, typically represented as a rectangular box and extended lines know as whiskers. The typical five numbers (corresponding to the five number summary) are the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. The box spans from Q1 to Q3 and represents the middle 50 percent of the data, with a line inside the box marking the median (Q2 value). The whiskers extend from each side of the box and may include the smallest and largest values. Some boxplots have heuristic cutoffs for the whiskers, where the minimum and maximum values are not captured within this range. Matplotlib uses the 1.5 times the interquartile range (IQR) convention for the whisker bounds, where the IQR is the distance between the first quartile (Q1) and the third quartile (Q3), when the minimum and maximum values extend beyond this range.\nLike histograms, boxplots can depict skewness and provide visual information about potential unusual observations or values. Boxplots are often used to assess and compare variation and measures of center across groups.\n\n# A boxplot of heights\n# NOTE: `vert=False` changes default display from vertical to horizontal. \n\nstarwars['height'].plot(kind = 'box', vert = False)\n\n\n\n\n\n\n\n\nLike the histogram of the same variable, the boxplot above shows the distribution of the characters’ heights, but also provides information about the distributional percentiles, or more specifically the quartiles. We can see that the median (Q2) height is approximately at 180 centimeters. We can also see that 50% of the characters have heights approximately between 165 (Q1) and 190 (Q3) centimeters. We can imagine overlaying this boxplot on the histogram and seeing the connections and similar information displayed by each visual.\nNext, let’s consider how heights might vary across human and other nonhuman character species. For this, we can filter the data using our most frequent species classifications, Human, Droid, and Gungan. Then we can generate side-by-side boxplots for each group to gain insight into and compare the distributions of heights across these species.\n\n# Define a list with the species to include\nspecies_groups = ['Human', 'Droid', 'Gungan']\n\n# Create a Boolean mask to filter for those species\nmask = starwars['species'].isin(species_groups)\n\n# Create a horizontal box plot of height by species\nstarwars[mask].boxplot(column = 'height', by = 'species', vert = False)\n\n\n\n\n\n\n\n\nIn the code above, the .isin() method (from pandas) checks whether each value in column is found within the specified list. For each column entry, if in the list the method returns True and otherwise, False. This result is a Boolean Series that can be used to filter the rows of a DataFrame.\nReturning to the visual, we can see that the distribution of heights across the three different species are very different. For example, there is almost no overlap between the heights represented by the boxplots. These differences were hidden in the boxplot and histogram for the entire set of characters. This highlights the need to also consider relationships between variables and how these associations might influence certain observation and interpretations.\nHowever, as we noted in observing the counts of each species, we should keep in mind the frequencies that are represented by the graphs. In particular, there are only three Gungans in the dataset. So, although the boxplot representing Gungan heights has the typical features, the underlying data from which it was generated consists of only a minimum, median, and maximum. Similarly, the Droids are represented by a mere 6 entries, which does not make for an interesting (or useful) five number summary. So, once again, we see that context is paramount in the interpretations of these visuals and the degree of confidence that you may want to assign to them. In fact, you may have seen other droids in The Mandalorian!\n\n\n6.3.4 Scatterplots\nBivariate scatterplots depict the ordered pairs of two numerical variables of interest as points on a coordinate plane. These visualizations can be used to investigate potential relationships between two variables and can be useful in visualizing patterns such as trends, clusters, and associations. Scatterplots can also show unusual observations and are a standard tool in many model diagnostic procedures, among other uses.\nWe can use a scatterplot to visualize the relationship between the Star Wars characters’ heights and masses.\n\n# A scatterplot of height vs. mass\n\nstarwars.plot(kind = 'scatter', x = 'height', y = 'mass') \n\n\n\n\n\n\n\n\nFrom the graph, there appears to be a linear relationship between mass and height. However, what may be an increasing trend is obscured by the horizontal access scale. This is due to the observation with an extreme mass value close to 1400 kilograms. Due to the scale distortion resulting from this single observation, we might want to examine how the picture changes if we were to exclude it.\n\n# Find the highest mass in the dataset\n\nmax_mass = starwars['mass'].max()\n\n# Create a Boolean mask that removes the character with the highest mass\n\nmask = starwars['mass'] != max_mass\n\n# Create a scatter plot without the outlier\n\nstarwars[mask].plot(kind = 'scatter', x = 'height', y = 'mass')\n\n\n\n\n\n\n\n\nUpon excluding the observation with the extreme mass value and re-plotting the points, the suspected increasing trend (i.e., as height increases, mass increases) is more clear. Now that the variation in mass is more discernible via the new visualization, we might even want to consider more dynamic models (e.g., curvilinear), that might better explain the relationship between characters’ heights and masses.\n\n\n6.3.5 Line Charts\nA two-dimensional line chart is a visual representation of data points (ordered pairs) connected by straight lines. These displays typically show numerical changes over an ordered interval. The horizontal axis is typically a numerical variable, a sequence, or an ordered set of meaningful categories.\nLine charts are often used to observe series and changes with respect to time. Line charts can also help visualize non-temporal directions of change with respect to categories like “beginner”, “intermediate”, and “advanced”; “low”, “medium”, “high” or other groupings with various levels. The points in the visualization can represent single observations or summary statistics, such as category/group means, corresponding to the associated horizontal axis labels.\n\n6.3.5.1 Line Chart Example\nThe Ramen Rater dataset consists of review beginning in 2002, but only has records of dates from 2010 onward. To examine how the frequency of instant noodle reviews has varied over the years we can use the ratings dataset (read in below). This dataset was created by extracting review numbers and corresponding dates from the Ramen Rater website. Unlike the Big List, which does not include date information, the ratings dataset enables grouping by year and allows for counting the number of reviews completed annually. This approach offers insight into how the Ramen Rater’s review activity has changed over time.\n\n# Load the Ramen Rater review data\n\nratings = pd.read_csv(\"ramen_reviews.csv\")\n\n# Group the data by year\n\ngrps = ratings.groupby('year')\n\n# Create a line chart, excluding the last year (2025) \n\ngrps.size()[:-1].plot(kind='line', marker='o')\n\n\n\n\n\n\n\n\nThe line chart above shows changes in the number of reviews over time. Specifically, we can observe a sharp increase from 2010 to 2011 from approximately 50 reviews to over 360 reviews. For the remainder of years (2012 - 2025), the number of reviews fluctuate between 250 and 390. In general, we can see a relatively consistent recording of reviews over the years.\n\n\n\n6.3.6 Customizing Visualizations\nThe matplotlib library provides a wide range of additional visualization customization options, including feature options such as adding titles and axis labels. Additional options, including some used above, include customized orientations, sizes, shapes, and colors. To explore the extensive list of customization options consult the Matplotlib documentation and consider leveraging the power of generative AI tools, too!\nNext, we return to R to explore more visualizations with an eye towards exploration and communication.\n\n\n\n\n1. Tufte ER. The visual display of quantitative information. Second. Cheshire, Connecticut: Graphics Press. (2001). https://www.edwardtufte.com/tufte/books_vdqi",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project Part 4.1: Visualizations & Trends in Python</span>"
    ]
  },
  {
    "objectID": "proj7R.html",
    "href": "proj7R.html",
    "title": "7  Project Part 4.2: Visualizations & Trends in R",
    "section": "",
    "text": "7.1 Base R plot features\nBase R has a plot function, which for standard plots is very easy to use. To illustrate the utility of the base R plot function in connection with the communicative power of data visualizations, let’s look at an example after a brief presentation of some background information.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Part 4.2: Visualizations & Trends in R</span>"
    ]
  },
  {
    "objectID": "proj7R.html#base-r-plot-features",
    "href": "proj7R.html#base-r-plot-features",
    "title": "7  Project Part 4.2: Visualizations & Trends in R",
    "section": "",
    "text": "7.1.1 Traffic Accidents - Example\nThe traffic accidents data set contains data compiled from the National Highway Traffic Safety Administration for incidents occurring in 2020. Variables range from vehicle information to characteristics of the drivers & passengers. The data, by itself, is a collection of various values and other bits of information. We could summarize this content with statistics, such as means and frequencies. However, we could consider another way to get a global summary of the data through creating a simple visualization. In particular, when data is related to location, and this information is available in the dataset, there is an opportunity to gain insights into data characteristics by way of a spatial representations.\nBelow, we will leverage the location information in the data in conjunction with the base R plot() function to demonstrate the power of data visualization and the ease of creating these in R. Furthermore, we’ll consider some avenues of additional inquiry inspired by the visualizations of the data.\nOne variable in the Traffic Accidents data, STATENAME, contains information on the state in which a particular accident occurred. Because the data may have multiple rows representing different people associated with a particular accident we need to use the ST_CASE variable - a unique identifier for each accident - to filter the data to one observation per accident. This will allow us to get the frequencies of accidents while avoiding over-counting accidents with multiple entries in the dataset. The table below represents accurate counts of accidents per state, based on the data, obtained through the filtering described here in addition to other data moves.\n\n\n\n\n\n\n\n\n\n\n\n\nState\nAccidents\n\n\n\n\nAlabama\n851\n\n\nAlaska\n53\n\n\nArizona\n965\n\n\nArkansas\n585\n\n\nCalifornia\n3555\n\n\nColorado\n573\n\n\nConnecticut\n279\n\n\nDelaware\n103\n\n\nDistrict of Columbia\n34\n\n\nFlorida\n3097\n\n\nGeorgia\n1520\n\n\nHawaii\n81\n\n\nIdaho\n188\n\n\nIllinois\n1086\n\n\nIndiana\n815\n\n\nIowa\n304\n\n\nKansas\n382\n\n\nKentucky\n709\n\n\nLouisiana\n760\n\n\nMaine\n151\n\n\nMaryland\n540\n\n\nMassachusetts\n327\n\n\nMichigan\n1010\n\n\nMinnesota\n369\n\n\nMississippi\n687\n\n\n\n\n\n\nState\nAccidents\n\n\n\n\nMissouri\n911\n\n\nMontana\n190\n\n\nNebraska\n217\n\n\nNevada\n293\n\n\nNew Hampshire\n98\n\n\nNew Jersey\n547\n\n\nNew Mexico\n364\n\n\nNew York\n963\n\n\nNorth Carolina\n1412\n\n\nNorth Dakota\n95\n\n\nOhio\n1151\n\n\nOklahoma\n596\n\n\nOregon\n461\n\n\nPennsylvania\n1058\n\n\nRhode Island\n66\n\n\nSouth Carolina\n962\n\n\nSouth Dakota\n132\n\n\nTennessee\n1119\n\n\nTexas\n3517\n\n\nUtah\n256\n\n\nVermont\n57\n\n\nVirginia\n794\n\n\nWashington\n524\n\n\nWest Virginia\n249\n\n\nWisconsin\n561\n\n\nWyoming\n113\n\n\n\n\n\n\n\n\n\n\nNotice that both Alaska and Hawaii are included in the states for which accidents were recorded. Now, let’s take a look at how we can create a visual of this table using the base R plot() function.\n\n# the base R plot function takes simple\n# (x,y) coordinates to generate a plot\n\nplot(Accidents$LONGITUDE, Accidents$LATITUDE)\n\n\n\n\n\n\n\n\nBy simply adding two numeric variables into the plot function we were able to create a relatively informative visual of the data. For example, we could look at this graph and use our prior knowledge of maps and geography to infer that the data represent events that occurred in the United States. Knowing the context of the graph, we could also say that car accidents occur in almost all parts of the U.S. In general, with location data we may be able to relate to the context through examining our hometown or current location (e.g., North Carolina) if, or when, represented.\nOn the other hand, there is so much room for improvement. For this, we can even implement many graphical enhancements with the basic plot function through adding additional parameters and by also considering our visualization guidelines from the previous chapter.\n\n# plot with labels, color, and transparency\nplot(Accidents$LONGITUDE, Accidents$LATITUDE, \n     main = \"Traffic Accidents\", # main title\n     xlab = \"Longitude\", # x-axis label\n     ylab = \"Latitude\", # y-axis label\n     col = alpha(\"orange\",0.25), # specifies a color and transparency value\n     pch = 1) # pch = 1 makes open circles \n\n\n\n\n\n\n\n\nIn the visualization above, we have gotten rid of the unnecessary clutter on our axis labels, added an informative title, and also added transparency to our (now orange) data points. The transparency also revels more information about the density of the accidents with respect to latitude and longitude.\nIn our guidelines, we also talked about how graph scales are important. Although the graph scales can be customized, the default scale ranges in R for numeric data depend on the ranges of the data points themselves. These ranges can have major influences on the usefulness and accuracy of a graphical representation. This influence is sometimes discovered or encountered in situations where there are one or more outliers in the data. For example, if most data is within a range of 1 to 10, and one data point is at 200, the corresponding visual will not display any variation in the majority of what could be your highly variable data. In this case, the differences between data points in the 1 and 10 region would not be distinguishable on a scale that ranges up to 200.\nAs with the hypothetical example above, although the traffic accidents data does not have what we would necessarily classify as outliers, including the accidents in Hawaii and Alaska impacts the graph in a similar, but less extreme, way. For an analysis including all states, we may have a need to display all of the related data. But let’s see how things change if we restrict the data to the continental U.S.\n\n\n\n\n\n\n\n\n\nWith the graph rescaled to only include data from the continental U.S., we can now more readily visually discern accidents density variation across the (continental) U.S. As noted in the code comments, we specified the graphing coordinates criteria using the model formula operator ~ to generate the graph… because we can (and because there are so many ways to do a thing in R!).\nFinally, let’s create one last graph to Zoom in a bit on the home state of this book, North Carolina.\n\n# NC subset\nAccidents_NC &lt;- \n  Accidents %&gt;%\n  filter(STATENAME == \"North Carolina\")\n\n# NC graph, accidents locations\nplot(LATITUDE ~ LONGITUDE, data = Accidents_NC,\n     main = \"Traffic Accidents in North Carolina (2020)\", \n     xlab = \"Longitude\", \n     ylab = \"Latitude\", \n     col = alpha(\"orange\",0.5), \n     pch = 1) \n\n\n\n\n\n\n\n\nPretty cool, right! In general, in base R:\n\nThe default graph generated for two numeric variables from a dataset is a scatterplot.\n\nAs seen above, scatterplots can be particularly informative when they’re related to location information and prior knowledge, but definitely have more general utility for various appropriate data types.\nNow, what about categorical variables in base R?\n\n# one variable, categorical\nplot(Accidents$STATENAME) # barchart\n\n\n\n\n\n\n\n\nUsing the base R plot() function, the default graph for one categorical variable is a barchart. And now, we enhance.\n\nplot(Accidents$STATENAME,\n        col = \"purple\", # fill bar colors w purple\n        border = \"orange\",  # Color bar borders orange\n        las = 2,    # Rotate x-axis labels vertically\n        cex.names = 0.5,  # Shrink x-axis labels\n        main = \"Traffic Accidents by State\",\n        xlab = \"State\")\n# Adjust y-axis label to be horizontal\nmtext(\"Number of Accidents\", side = 2, line = 3, las = 0)\n\n\n\n\n\n\n\n\nBased on the graphical representation, we might infer that the number of accidents in states are correlated with the respective state populations, or number of drivers within a state. Although that may be an obvious starting point, we might want to see if the proportion of drivers in each state is significantly different from the proportions of drivers, perhaps with a plan to investigate why potential differences may exist.\nFor two categorical variables as inputs into the base R plot function, the graph that is generated is what is known as a mosaic plot. Although this can be generated in the same way as previous examples, through the base R plot() function, we will use more useful packages to create such a visual where we can more easily modify the labels and colors to create an informative display.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Part 4.2: Visualizations & Trends in R</span>"
    ]
  },
  {
    "objectID": "proj7R.html#other-common-plots",
    "href": "proj7R.html#other-common-plots",
    "title": "7  Project Part 4.2: Visualizations & Trends in R",
    "section": "7.2 Other common plots",
    "text": "7.2 Other common plots\nIn R, boxplots and histograms are just as easy to implement. Let’s look at the syntax for these visuals through a few examples.\n\nboxplot(Marine6$Depth) # distribution of depth\n\n\n\n\n\n\n\n\nWe could also create multiple boxplots for comparison by using model syntax with one numeric and one categorical variable (e.g., to compare distributions of depth by region).\n\nboxplot(Depth~Region, data=Marine6,\n        col = \"purple\", # fill bar colors w purple\n        border = \"orange\",  # Color bar borders orange\n        main = \"Depth by Region\",\n        cex.axis = 0.45, #shrink axis labels\n        xlab = \" \",\n        ylab = \" \",\n        las = 2 # Rotate x-axis labels vertically\n       )\nmtext(\"Depth (feet)\", side = 2, line = 3, las = 0)\n\n\n\n\n\n\n\n\nFrom this plot, we might want to investigate the factors that contribute to the visually evident differences in depth across the region.\nFinally, to generate a histogram we can use the function hist() in the code below.\n\nhist(Marine6$Depth)\n\n\n\n\n\n\n\n\nAs with the other plot functions in base R, we could modify the histogram to include more bins, various colors, labels, and other features. However, when we’re interested in customizing visualizations in R, the package to turn to is ggplot2!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Part 4.2: Visualizations & Trends in R</span>"
    ]
  },
  {
    "objectID": "proj7R.html#ggplot2",
    "href": "proj7R.html#ggplot2",
    "title": "7  Project Part 4.2: Visualizations & Trends in R",
    "section": "7.3 ggplot2!",
    "text": "7.3 ggplot2!\nOne of the best data visualization packages in R, and in general, is ggplot2 from the tidyverse. The ggplot2 syntax allows for building a variety of standard and customized graphs through layers. Similarly to features available in base R, the methods for visualizations built through ggplot2 include aesthetics such as groups, colors, opacity, size and more - with the bonus of minimal code and access to excellent documentation. As with other libraries within the tidyverse, ggplot2 has a very useful resource that can be found here: ggplot2 Cheat Sheet.\nWith the ggplot2 package, we will expand our visualizations to show examples of useful multivariate graphs, including mosaic plots and spaghetti plots.\n\n7.3.1 Examples with ggplot2\nThe Rising Global Temperature dataset consists of three variables, gistemp, year, and ci95.\n\ngistemp represents yearly-average temperature differences from the average of 1986 - 2005, for each year including and between 1880 to 2018.\n\nFor example, a gistemp measurement of 0.2 for a given year would represent a yearly average temperature of 0.2 degrees Celsius warmer than the average temperatures from 1986 - 2005.\n\nyear represents the year in which the temperatures were taken, and\nci95 represents the 95% confidence interval margins of error.\n\nBelow, we’ll use the ggplot2 package to visualize the temperature changes over time.\n\n# RGT is the Rising Global Temperature dataset\n# the aes() function indicates the variables to be plotted w.r.t. the axes\n# geom_point() adds the points layer to the visual\n# geom_line() adds the line layer to the visual\n# theme_minimal is an option that minimizes background items, such as grid lines\n# NOTE, each layer is added/connected to the next through the `+` operator\n\nRGT %&gt;% \n  ggplot(aes(x = year, y = gistemp)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom this plot, we can discern a generally increasing trend in the temperature differences over time. Patterns revealed here may not be so obvious in numerical summaries alone, especially in terms of concisely communicating or presenting the trend to a general audience. Considering communication efficacy, we may see some opportunities to add some customization to our graph to enhance the quality and consider additional information in the dataset. For example, we could visualize the confidence interval information that accompanies each year measured, make our axis labels more informative and add color. We do this in the following visual.\n\n# The alpha parameter adjust the opacity of the corresponding geom\n# We can improve our various labels through the labs() function\n\nlongtemp %&gt;%\n  ggplot(aes(x = year, y = temperature_difference, color = category)) +\n  geom_point(alpha = 0.5) +\n  geom_line() +\n  labs(x = \"Year\", \n       y = \"Temperature Difference\", \n       color = \"Category\", \n       title = \"Rising Global Temperature\",) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNote that we referenced a dataset called longtemp in the last code block. This dataset was derived from RGT via a series of data moves that included creating new variables and creating hierarchy through nesting upper bounds, lower bounds, averages, and the corresponding category information within each year (although explicitly repeated in the dataset).\nHere is an example of the new data structure needed for the graph.\n\n\n# A tibble: 6 × 3\n   year category temperature_difference\n  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n1  1880 average                  -0.607\n2  1880 upper                    -0.468\n3  1880 lower                    -0.746\n4  1881 average                  -0.518\n5  1881 upper                    -0.383\n6  1881 lower                    -0.652\n\n\nThe graph corresponding to longtemp is also an example of a spaghetti plot, which is an extension of a line plot to include group (or category) information. These graphs are often used to depict longitudinal data but can be useful for conveying many group-based patterns.\nNext, we make a few adjustments to present the trends in the Rising Global Temperatures data with a little less clutter (by removing the points), and with an additional line at our comparison average (where the temperature change is zero).\n\nlongtemp %&gt;%\n  ggplot(aes(x = year, y = temperature_difference, \n             color = category)) +\n  #geom_point(alpha = 0.5) +  (commented out, easy...)\n  geom_line() +\n  labs(x = \"Year\", \n       y = \"Temperature Difference\", \n       color = \"Category\",\n       title = \"Rising Global Temperature\",) +\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color=\"purple\", alpha = .6)\n\n\n\n\n\n\n\n\nAs you can see by comparing the code blocks, modifying visualizations through ggplot is quite convenient. In particular, we can overlay additional features by simply adding the relevant geom to the visualization.\n\n\n\n\n\n\nDesign Practices\n\n\n\nWhen you create data visualizations, there are many choices that can be made. Considerations for user experience, for example, can help to guide your choices. These may include choosing accessible color palettes and adding descriptive text for a given visualization.\nCompare the code and corresponding images from the visuals above and below. How do you imagine the two different graphs might impact perception?\n\n\n\nlongtemp %&gt;%\n  ggplot(aes(x = year, y = temperature_difference, \n             color = category, linetype = category)) +\n  #geom_point(alpha = 0.5) +  (commented out, easy...)\n  geom_line() +\n  labs(x = \"Year\", \n       y = \"Temperature Difference\", \n       color = \"Category\",\n       linetype = \"Category\",\n       title = \"Rising Global Temperature\",) +\n  theme_minimal() +\n  theme(legend.position = \"top\",\n        legend.title = element_blank()) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color=\"purple\", alpha = .6)\n\n\n\n\n\n\n\n\n\n\n7.3.2 Faceting\nAs an alternative to plotting multiple groups on one graph, we may want to compare groups on the same scale on different graphs within the same visualization. We have a convenient way to do this through adding facet options. We can see an example of this using our familiar Marine6 dataset.\nFor the following visualization, we used data moves (e.g, filtering, grouping, creating new variables) on the Marine6 dataset to calculate the average depth of the marine species for each year, and created a new dataset called AvgDepth_Year, referenced below.\n\nAvgDepth_Year %&gt;%\n  ggplot(aes(x = Year, y = `Average Depth`, \n             group = `Marine Species`, color = `Marine Species`)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"top\") +\n  labs(title = \"Average Depth Over Time\", y = \"Depth (feet)\")\n\n\n\n\n\n\n\n\nNow, let’s employ the facet option to see what this can do.\n\nAvgDepth_Year %&gt;%\n  ggplot(aes(x = Year, y = `Average Depth`, \n             group = `Marine Species`, color = `Marine Species`))  +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ `Marine Species`) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 4, angle = 90, vjust = 0.5, hjust=1),\n        legend.position = \"top\") +\n  labs(title = \"Average Depth Over Time\", y = \"Depth (feet)\")\n\n\n\n\n\n\n\n\n\n\n7.3.3 Categorical Example\nFinally, let’s check out an example of a useful plot for two categorical variables, a mosaic plot. Mosaic plots can yield insights into patterns of association, bivariate frequencies and more. Although this can be done in base R, the ggmosaic package is designed for this purpose and uses the same useful structure as ggplot2.\nIn the visual below, we have created a dataset Mar6 from the Marine6 data via filtering and creating new variables. ggmosaic enters the ggplot syntax as a geom object with parameters as seen in the code below. Following the addition of the mosaic geom, we customized the graph features to capture the features of interest in a concise and quality way. You may even notice a potential association from examining the visual. Could this visual help you tell a data story?\n\nlibrary(ggmosaic)\n\n# let's visualize a contingency table...\n\n# NOTE, ggmosaic requires a base R data.frame() object until it is updated to accommodate tibbles.\n\nMar6 %&gt;% \n  data.frame() %&gt;%\n  ggplot() +\n  geom_mosaic(aes(x = product(Common.Name), fill = Depth_Range)) +\n  theme_mosaic() +\n  scale_fill_manual(values = c(\"#FFCA99\",\"#1E8E99\",\"#99F9FF\",\"#CCFEFF\"),\n                    guide = guide_legend(reverse = TRUE)) +\n  labs(\n    x = \" \",\n    y = \" \",\n    title = \"Is depth associated with marine species?\",\n    fill = \"Depth Range\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_text(size = 7),\n    axis.title.y = element_text(size = 8),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 7),\n    plot.title = element_text(size = 10, hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Moves for Data Viz\n\n\n\nData moves often go hand in hand with preparing data for visualizations. In this chapter, each visualization was preceded by data processing steps so that the information we wished to display was available in the right format for the generating functions. Understanding your data and related data manipulation processes (e.g., data moves) are essential to creating informative data depictions. Here, once again, we see (and visualize) the interconnectedness of the data science workflow!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Part 4.2: Visualizations & Trends in R</span>"
    ]
  },
  {
    "objectID": "proj8.html",
    "href": "proj8.html",
    "title": "8  Project Part 5: Communicating results",
    "section": "",
    "text": "8.1 Presentation Guidelines\nThe bullet points below are the guidelines for your presentation. These considerations can be used beyond this project, more generally, as big picture concepts and explanatory needs for building and communicating a data investigation.\nThe data description should include important contextual and background information. This is essentially a summary of your data dictionary.\nAlthough many data investigations aren’t necessarily interest driven, making data connections through interest and relevance can be a powerful tool to boost, motivate and/or facilitate understanding. In some cases, identifying interests in the data or using interests to drive a data investigation can allow you to access your prior knowledge and experience. Or, perhaps you chose the data because you were interested in investigating something new.\nDuring this step, you should describe any hypotheses you generated or expectations that you may have had going into the investigations. Relatedly, you should describe your dataset characteristics in terms of your data investigation. This step might be different from your previous data descriptions in that you might desrcibe a subset of your data in terms of the hypotheses and questions you seek to investigate. Here, you might include more information about the meaning and measures of certain variables. Additionally, you may have visualizations or tables that help you describe aspects of the data relevant to your investigation.\nFor this step you can elaborate on the why behind your data filtering or subsetting choices (among other data processing choices). You might explain why you used averages, or why you created a new variable (e.g., created levels from a continuous variable, merged your supplemental data by a common ID, etc.).\nFor this step, describe what your analysis was and why you chose such a method. It’s possible that your analysis step required additional data moves. For example, if you decided to visualize a trend in the data, perhaps you had to reformat information (as seen in previous chapters) in order to create your desired data representation.\nIn communicating your findings, you should provide a contextual explanation. Or, if your data investigation and the related analysis methods were exploratory, you might describe the insight you found that require further investigation, perhaps including suggestions for additional needs and lines of inquiry. These might include recommendations for additional measures and sources that would allow for answering the related questions.\nThe need for extra information is also related to the limitations of your data. There may be limitations to the generalizability of your findings and the conclusions that can be garnered. For example, a visual trend might be a strong suggestion for a particular relationship, but the strength of the relationship and related statements you might want to make could be further supported by formal statistical modeling, analysis, and related diagnostics.\nFor this project, you should reflect on your process and connect it to data science and project workflows. You might look back at chapter one to see how your process aligned or differed from the frameworks presented there. Consider and explain where you might use AI in a particular project or workflow phase that you may not have already. Consider what you might do differently in a new individual project, or in a project team setting. Perhaps you have ideas for your personal data science workflow philosophy!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Part 5: Communicating results</span>"
    ]
  },
  {
    "objectID": "proj8.html#presentation-guidelines",
    "href": "proj8.html#presentation-guidelines",
    "title": "8  Project Part 5: Communicating results",
    "section": "",
    "text": "Describe your data\n\n\n\nExplain what interests you about the dataset & why you chose it.\n\n\n\nExplain your data investigation or research question of interest?\n\n\n\nExplain how you processed your data (e.g, tell us about your data moves).\n\n\n\nExplain what you did to investigate your question of interest?\n\n\n\nExplain what you found.\n\n\n\n\nDescribe how this project relates to (or informs, or does not relate to or inform) your idea of data science & programming.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Part 5: Communicating results</span>"
    ]
  },
  {
    "objectID": "proj8.html#what-else",
    "href": "proj8.html#what-else",
    "title": "8  Project Part 5: Communicating results",
    "section": "8.2 What else?",
    "text": "8.2 What else?\nThe format in which you choose to present your information is important. Many of the components above can be reported directly from default programming output. However, you should not just present the default output from something like the summary() function in R, for example. This information is undoubtedly useful, but you should reformat this type of output to include what’s relevant and important with the same considerations you might have for a data visualization (e.g., minimizing clutter, adding contrast between rows of information, etc.).\nLikewise, you should not include all of your code in your presentation. For example, there would not be a need to show your coding process for creating a particular data visualization. In fact, you might choose to hide all of your code within your presentation. You would have your (well commented) code accessible apart from your presentation as part of the reproducibility process and, in this case, as a course requirement.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Part 5: Communicating results</span>"
    ]
  },
  {
    "objectID": "proj8.html#a-few-resources",
    "href": "proj8.html#a-few-resources",
    "title": "8  Project Part 5: Communicating results",
    "section": "8.3 A few resources",
    "text": "8.3 A few resources\n\nIf you choose to create your presentation within quarto, check out this resource: Quarto Presentations.\nTo create formatted tables, consider the kableExtra package",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Part 5: Communicating results</span>"
    ]
  },
  {
    "objectID": "proj8.html#finally",
    "href": "proj8.html#finally",
    "title": "8  Project Part 5: Communicating results",
    "section": "8.4 Finally",
    "text": "8.4 Finally\nCongratulations on learning to use R and Python within a data science workflow. You’re now ready for your next step in your data science and AI learning journey!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project Part 5: Communicating results</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Lee\nH, Mojica G, Thrasher E, Baumgartner P. Investigating data like a data\nscientist: Key practices and processes. Statistics Education\nResearch Journal (2022) 21:3–3.\n\n\n2. Council FP. Fair information practice\nprinciples. (n.d.)\n\n\n3. Erickson T, Wilkerson M, Finzer W, Reichsman F.\nData moves. Technology Innovations in Statistics Education\n(2019) 12:\n\n\n4. Pruim R, Gı̂rjău M-C, Horton NJ. Fostering\nbetter coding practices for data scientists. Harvard Data Science\nReview (2023) 5:\n\n\n5. Tufte ER. The visual display of\nquantitative information. Second. Cheshire, Connecticut: Graphics\nPress. (2001). https://www.edwardtufte.com/tufte/books_vdqi",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix-a.html",
    "href": "appendix-a.html",
    "title": "Appendix A — The ADAPT Model",
    "section": "",
    "text": "A.1 Components of the Model",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ADAPT Model</span>"
    ]
  },
  {
    "objectID": "appendix-a.html#components-of-the-model",
    "href": "appendix-a.html#components-of-the-model",
    "title": "Appendix A — The ADAPT Model",
    "section": "",
    "text": "The ADAPT Model Components\n\n\n\nA.1.1 The 10 Common Learning Elements\nData Perspectives\n\nRecognizing data as information – not truth – with error, variability, and missing information,\nExplaining what it means to be a data scientist and AI expert,\nObserving a variety of data scientist role models and careers.\n\nData Practices\n\nExamining how data are created, and the related assumptions and collection practices,\nPracticing data curation, wrangling, and cleaning,\nAssessing validity of data, methods, results, and communication,\nEmploying design practices such as documenting work, considering whether broadband is required for applications, including color palettes that are visible to people who are color blind, adding captions to video and adding descriptive text to images,\nInvestigating ethical issues and ways to approach them.\n\nData Discoveries\n\nArticulating current issues or open questions in data science, and\nSpecifying exciting discoveries or impacts of data science.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ADAPT Model</span>"
    ]
  }
]